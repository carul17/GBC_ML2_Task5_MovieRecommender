{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Recommender Systems\n",
    "\n",
    "This notebook implements various movie recommendation approaches:\n",
    "1. Popularity-based\n",
    "2. Content-based Filtering\n",
    "3. Collaborative Filtering\n",
    "4. Matrix Factorization\n",
    "5. Hybrid Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from surprise import Dataset, Reader, SVD\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "from functools import lru_cache\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from scipy import sparse\n",
    "import h5py\n",
    "import joblib\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Metal device: mps\n"
     ]
    }
   ],
   "source": [
    "# Setup device and optimizations for Apple Silicon\n",
    "device = torch.device('mps')\n",
    "torch.backends.mps.enable_fallback_kernels = True\n",
    "print(f\"Using Apple Metal device: {device}\")\n",
    "\n",
    "# Advanced caching mechanism\n",
    "class RecommendationCache:\n",
    "    def __init__(self, max_size=1000):\n",
    "        self.cache = {}\n",
    "        self.max_size = max_size\n",
    "    \n",
    "    @lru_cache(maxsize=1000)\n",
    "    def get_movie_features(self, movie_idx):\n",
    "        return self.latent_matrix_gpu[movie_idx]\n",
    "    \n",
    "    def get(self, key):\n",
    "        return self.cache.get(key)\n",
    "    \n",
    "    def set(self, key, value):\n",
    "        if len(self.cache) >= self.max_size:\n",
    "            self.cache.pop(next(iter(self.cache)))\n",
    "        self.cache[key] = value\n",
    "\n",
    "cache = RecommendationCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading movies...\n",
      "Loading ratings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading ml-20m/ratings.csv: 100%|██████████| 20000263/20000263 [00:02<00:00, 9760186.54it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tags...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading ml-20m/tags.csv: 100%|██████████| 465564/465564 [00:00<00:00, 4509666.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing tags...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sequence item 66: expected str instance, float found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/George Brown Courses/machine_learning_2/Task5/.venv/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1824\u001b[39m, in \u001b[36mGroupBy.apply\u001b[39m\u001b[34m(self, func, include_groups, *args, **kwargs)\u001b[39m\n\u001b[32m   1823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1824\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_python_apply_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selected_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1825\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1826\u001b[39m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.obj, Series)\n\u001b[32m   1827\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._selection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1828\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._selected_obj.shape != \u001b[38;5;28mself\u001b[39m._obj_with_exclusions.shape\n\u001b[32m   1829\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/George Brown Courses/machine_learning_2/Task5/.venv/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1885\u001b[39m, in \u001b[36mGroupBy._python_apply_general\u001b[39m\u001b[34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[39m\n\u001b[32m   1859\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1860\u001b[39m \u001b[33;03mApply function f in python space\u001b[39;00m\n\u001b[32m   1861\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1883\u001b[39m \u001b[33;03m    data after applying f\u001b[39;00m\n\u001b[32m   1884\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1885\u001b[39m values, mutated = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_grouper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_groupwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/George Brown Courses/machine_learning_2/Task5/.venv/lib/python3.11/site-packages/pandas/core/groupby/ops.py:919\u001b[39m, in \u001b[36mBaseGrouper.apply_groupwise\u001b[39m\u001b[34m(self, f, data, axis)\u001b[39m\n\u001b[32m    918\u001b[39m group_axes = group.axes\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m res = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[179]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mProcessing tags...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     37\u001b[39m tags_grouped = tags_df.groupby(\u001b[33m'\u001b[39m\u001b[33mmovieId\u001b[39m\u001b[33m'\u001b[39m)[\u001b[33m'\u001b[39m\u001b[33mtag\u001b[39m\u001b[33m'\u001b[39m].apply(\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m.join(x[:\u001b[32m1000\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) > \u001b[32m1000\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x)\n\u001b[32m     39\u001b[39m ).reset_index()\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Merge and clean up\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: sequence item 66: expected str instance, float found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[179]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Process tags in chunks\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mProcessing tags...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m tags_grouped = \u001b[43mtags_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmovieId\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtag\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m)\u001b[49m.reset_index()\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Merge and clean up\u001b[39;00m\n\u001b[32m     42\u001b[39m movies_with_tags = pd.merge(movies_df, tags_grouped, on=\u001b[33m'\u001b[39m\u001b[33mmovieId\u001b[39m\u001b[33m'\u001b[39m, how=\u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/George Brown Courses/machine_learning_2/Task5/.venv/lib/python3.11/site-packages/pandas/core/groupby/generic.py:230\u001b[39m, in \u001b[36mSeriesGroupBy.apply\u001b[39m\u001b[34m(self, func, *args, **kwargs)\u001b[39m\n\u001b[32m    224\u001b[39m \u001b[38;5;129m@Appender\u001b[39m(\n\u001b[32m    225\u001b[39m     _apply_docs[\u001b[33m\"\u001b[39m\u001b[33mtemplate\u001b[39m\u001b[33m\"\u001b[39m].format(\n\u001b[32m    226\u001b[39m         \u001b[38;5;28minput\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mseries\u001b[39m\u001b[33m\"\u001b[39m, examples=_apply_docs[\u001b[33m\"\u001b[39m\u001b[33mseries_examples\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    227\u001b[39m     )\n\u001b[32m    228\u001b[39m )\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, *args, **kwargs) -> Series:\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/George Brown Courses/machine_learning_2/Task5/.venv/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1846\u001b[39m, in \u001b[36mGroupBy.apply\u001b[39m\u001b[34m(self, func, include_groups, *args, **kwargs)\u001b[39m\n\u001b[32m   1830\u001b[39m             warnings.warn(\n\u001b[32m   1831\u001b[39m                 message=_apply_groupings_depr.format(\n\u001b[32m   1832\u001b[39m                     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mapply\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1835\u001b[39m                 stacklevel=find_stack_level(),\n\u001b[32m   1836\u001b[39m             )\n\u001b[32m   1837\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   1838\u001b[39m         \u001b[38;5;66;03m# gh-20949\u001b[39;00m\n\u001b[32m   1839\u001b[39m         \u001b[38;5;66;03m# try again, with .apply acting as a filtering\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1843\u001b[39m         \u001b[38;5;66;03m# fails on *some* columns, e.g. a numeric operation\u001b[39;00m\n\u001b[32m   1844\u001b[39m         \u001b[38;5;66;03m# on a string grouper column\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1846\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_python_apply_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_obj_with_exclusions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1848\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/George Brown Courses/machine_learning_2/Task5/.venv/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1885\u001b[39m, in \u001b[36mGroupBy._python_apply_general\u001b[39m\u001b[34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[39m\n\u001b[32m   1850\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m   1851\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_python_apply_general\u001b[39m(\n\u001b[32m   1852\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1857\u001b[39m     is_agg: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1858\u001b[39m ) -> NDFrameT:\n\u001b[32m   1859\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1860\u001b[39m \u001b[33;03m    Apply function f in python space\u001b[39;00m\n\u001b[32m   1861\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1883\u001b[39m \u001b[33;03m        data after applying f\u001b[39;00m\n\u001b[32m   1884\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1885\u001b[39m     values, mutated = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_grouper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_groupwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1886\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1887\u001b[39m         not_indexed_same = mutated\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/George Brown Courses/machine_learning_2/Task5/.venv/lib/python3.11/site-packages/pandas/core/groupby/ops.py:919\u001b[39m, in \u001b[36mBaseGrouper.apply_groupwise\u001b[39m\u001b[34m(self, f, data, axis)\u001b[39m\n\u001b[32m    917\u001b[39m \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n\u001b[32m    918\u001b[39m group_axes = group.axes\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m res = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[32m    921\u001b[39m     mutated = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[179]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Process tags in chunks\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mProcessing tags...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     37\u001b[39m tags_grouped = tags_df.groupby(\u001b[33m'\u001b[39m\u001b[33mmovieId\u001b[39m\u001b[33m'\u001b[39m)[\u001b[33m'\u001b[39m\u001b[33mtag\u001b[39m\u001b[33m'\u001b[39m].apply(\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m.join(x[:\u001b[32m1000\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) > \u001b[32m1000\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x)\n\u001b[32m     39\u001b[39m ).reset_index()\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Merge and clean up\u001b[39;00m\n\u001b[32m     42\u001b[39m movies_with_tags = pd.merge(movies_df, tags_grouped, on=\u001b[33m'\u001b[39m\u001b[33mmovieId\u001b[39m\u001b[33m'\u001b[39m, how=\u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: sequence item 66: expected str instance, float found"
     ]
    }
   ],
   "source": [
    "# Calculate chunk size based on available memory\n",
    "def get_optimal_chunk_size():\n",
    "    available_mem = psutil.virtual_memory().available\n",
    "    # Use 20% of available memory for chunk size\n",
    "    return int((available_mem * 0.2) / (8 * 1024))\n",
    "\n",
    "# Read datasets in chunks with disk caching\n",
    "def read_chunked_csv(filename, chunksize=None):\n",
    "    cache_file = Path(f\".cache_{Path(filename).stem}.joblib\")\n",
    "    if cache_file.exists():\n",
    "        return joblib.load(cache_file)\n",
    "    \n",
    "    chunksize = chunksize or get_optimal_chunk_size()\n",
    "    chunks = []\n",
    "    total_rows = sum(1 for _ in open(filename)) - 1  # Subtract header\n",
    "    \n",
    "    with tqdm(total=total_rows, desc=f\"Loading {filename}\") as pbar:\n",
    "        for chunk in pd.read_csv(filename, chunksize=chunksize):\n",
    "            chunks.append(chunk)\n",
    "            pbar.update(len(chunk))\n",
    "    \n",
    "    result = pd.concat(chunks)\n",
    "    joblib.dump(result, cache_file)\n",
    "    return result\n",
    "\n",
    "print(\"Loading movies...\")\n",
    "movies_df = pd.read_csv('ml-20m/movies.csv')\n",
    "\n",
    "print(\"Loading ratings...\")\n",
    "ratings_df = read_chunked_csv('ml-20m/ratings.csv')\n",
    "\n",
    "print(\"Loading tags...\")\n",
    "tags_df = read_chunked_csv('ml-20m/tags.csv')\n",
    "\n",
    "# Process tags in chunks\n",
    "print(\"Processing tags...\")\n",
    "tags_df['tag'] = tags_df['tag'].fillna('').astype(str)\n",
    "tags_grouped = tags_df.groupby('movieId')['tag'].apply(\n",
    "    lambda x: ' '.join(x[:1000] if len(x) > 1000 else x)\n",
    ").reset_index()\n",
    "\n",
    "# Merge and clean up\n",
    "movies_with_tags = pd.merge(movies_df, tags_grouped, on='movieId', how='left')\n",
    "movies_with_tags['tag'] = movies_with_tags['tag'].fillna('')\n",
    "\n",
    "# Clean up memory\n",
    "del tags_df, tags_grouped\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Popularity-based Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def popularity_recommender(n_recommendations=10):\n",
    "    # Calculate mean rating and number of ratings for each movie\n",
    "    movie_stats = ratings_df.groupby('movieId').agg({\n",
    "        'rating': ['count', 'mean']\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Rename columns\n",
    "    movie_stats.columns = ['movieId', 'rating_count', 'rating_mean']\n",
    "    \n",
    "    # Filter movies with minimum number of ratings (e.g., 100)\n",
    "    popular_movies = movie_stats[movie_stats['rating_count'] >= 100]\n",
    "    \n",
    "    # Sort by rating mean and count\n",
    "    popular_movies = popular_movies.sort_values(['rating_mean', 'rating_count'], ascending=[False, False])\n",
    "    \n",
    "    # Get movie titles\n",
    "    recommendations = pd.merge(popular_movies, movies_df, on='movieId')\n",
    "    \n",
    "    return recommendations[['movieId', 'title', 'rating_mean', 'rating_count', 'genres']].head(n_recommendations) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Content-based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing TF-IDF in batches...\n",
      "Creating TF-IDF vectors...\n",
      "Fitting TF-IDF vocabulary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing TF-IDF batches: 100%|██████████| 10/10 [00:00<00:00, 10.56it/s]\n"
     ]
    }
   ],
   "source": [
    "def process_tfidf_in_batches(texts, batch_size=1000):\n",
    "    cache_file = Path(\".cache_tfidf.h5\")\n",
    "    if cache_file.exists():\n",
    "        with h5py.File(cache_file, 'r') as f:\n",
    "            return torch.tensor(f['latent_matrix'][()], device=device)\n",
    "    \n",
    "    print(\"Creating TF-IDF vectors...\")\n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "    \n",
    "    # First pass to fit vocabulary\n",
    "    print(\"Fitting TF-IDF vocabulary...\")\n",
    "    tfidf.fit(texts)\n",
    "    \n",
    "    # Process in batches\n",
    "    n_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "    latent_matrices = []\n",
    "    \n",
    "    for i in tqdm(range(n_batches), desc=\"Processing TF-IDF batches\"):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(texts))\n",
    "        batch_texts = texts[start_idx:end_idx]\n",
    "        \n",
    "        # Transform batch and keep sparse\n",
    "        batch_tfidf = tfidf.transform(batch_texts)\n",
    "        \n",
    "        # Compute SVD for batch with explicit float32 dtype for MPS compatibility\n",
    "        batch_U, batch_S, batch_V = torch.svd(\n",
    "            torch.tensor(batch_tfidf.toarray(), device=device, dtype=torch.float32)\n",
    "        )\n",
    "        batch_latent = (batch_U[:, :100] @ torch.diag(batch_S[:100]))\n",
    "        latent_matrices.append(batch_latent.cpu().numpy())\n",
    "    \n",
    "    # Combine results\n",
    "    latent_matrix = np.vstack(latent_matrices)\n",
    "    \n",
    "    # Cache results\n",
    "    with h5py.File(cache_file, 'w') as f:\n",
    "        f.create_dataset('latent_matrix', data=latent_matrix)\n",
    "    \n",
    "    return torch.tensor(latent_matrix, device=device)\n",
    "\n",
    "print(\"Processing TF-IDF in batches...\")\n",
    "latent_matrix_gpu = process_tfidf_in_batches(movies_with_tags['tag'])\n",
    "\n",
    "# Cache for frequently accessed movie features\n",
    "@lru_cache(maxsize=1000)\n",
    "def get_cached_movie_features(movie_idx):\n",
    "    return latent_matrix_gpu[movie_idx]\n",
    "\n",
    "def content_based_recommender(movie_title, n_recommendations=10):\n",
    "    # Get movie index and features from cache\n",
    "    movie_idx = movies_with_tags[movies_with_tags['title'] == movie_title].index[0]\n",
    "    query_vector = get_cached_movie_features(movie_idx)\n",
    "    \n",
    "    # Batch compute similarities using optimized operations\n",
    "    similarities = torch.nn.functional.cosine_similarity(\n",
    "        query_vector.unsqueeze(0).unsqueeze(0),\n",
    "        latent_matrix_gpu.unsqueeze(0)\n",
    "    ).squeeze()\n",
    "    \n",
    "    # Get top recommendations using MPS-optimized topk\n",
    "    _, similar_indices = similarities.topk(n_recommendations + 1)\n",
    "    similar_indices = similar_indices[1:].cpu().numpy()\n",
    "    \n",
    "    # Cache the results\n",
    "    cache.set(f\"content_{movie_title}\", similar_indices)\n",
    "    \n",
    "    recommendations = movies_with_tags.iloc[similar_indices][['movieId', 'title', 'genres']]                                                                      \n",
    "    recommendations['title'] = recommendations['title'].str.ljust(50)                                                                                   \n",
    "    return recommendations  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating user-movie matrix...\n",
      "Creating sparse user-movie matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building sparse matrix: 100%|██████████| 100836/100836 [00:00<00:00, 108230.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing collaborative filtering SVD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SVD Progress: 100%|██████████| 1/1 [00:00<00:00,  9.34it/s]\n"
     ]
    }
   ],
   "source": [
    "def create_sparse_matrix(ratings_df, batch_size=100000):\n",
    "    print(\"Creating sparse user-movie matrix...\")\n",
    "    cache_file = Path(\".cache_collab.npz\")\n",
    "    cache_maps = Path(\".cache_collab_maps.joblib\")\n",
    "    \n",
    "    if cache_file.exists() and cache_maps.exists():\n",
    "        matrix = sparse.load_npz(cache_file)\n",
    "        cache_data = joblib.load(cache_maps)\n",
    "        if len(cache_data) == 2:  # Old cache format\n",
    "            user_map, movie_map = cache_data\n",
    "            movie_ids = list(movie_map.keys())\n",
    "        else:  # New cache format\n",
    "            user_map, movie_map, movie_ids = cache_data\n",
    "        return matrix, user_map, movie_map, movie_ids\n",
    "    \n",
    "    rows, cols, data = [], [], []\n",
    "    user_map = {}\n",
    "    movie_map = {}\n",
    "    movie_ids = []\n",
    "    \n",
    "    # Process in batches\n",
    "    total_rows = len(ratings_df)\n",
    "    with tqdm(total=total_rows, desc=\"Building sparse matrix\") as pbar:\n",
    "        for start in range(0, total_rows, batch_size):\n",
    "            batch = ratings_df.iloc[start:start + batch_size]\n",
    "            \n",
    "            for _, row in batch.iterrows():\n",
    "                if row['userId'] not in user_map:\n",
    "                    user_map[row['userId']] = len(user_map)\n",
    "                if row['movieId'] not in movie_map:\n",
    "                    movie_map[row['movieId']] = len(movie_map)\n",
    "                    movie_ids.append(row['movieId'])\n",
    "                \n",
    "                rows.append(user_map[row['userId']])\n",
    "                cols.append(movie_map[row['movieId']])\n",
    "                data.append(row['rating'])\n",
    "                \n",
    "            pbar.update(len(batch))\n",
    "    \n",
    "    matrix = sparse.csr_matrix((data, (rows, cols)), \n",
    "                              shape=(len(user_map), len(movie_map)))\n",
    "    \n",
    "    # Cache the results\n",
    "    sparse.save_npz(cache_file, matrix)\n",
    "    joblib.dump((user_map, movie_map, movie_ids), cache_maps)\n",
    "    \n",
    "    return matrix, user_map, movie_map, movie_ids\n",
    "\n",
    "print(\"Creating user-movie matrix...\")\n",
    "user_movie_matrix, user_map, movie_map, movie_ids = create_sparse_matrix(ratings_df)\n",
    "\n",
    "print(\"Performing collaborative filtering SVD...\")\n",
    "with tqdm(total=1, desc=\"SVD Progress\") as pbar:\n",
    "    svd_collab = TruncatedSVD(n_components=100)\n",
    "    latent_matrix_2 = svd_collab.fit_transform(user_movie_matrix)\n",
    "    pbar.update(1)\n",
    "\n",
    "# Convert matrices to GPU tensors once\n",
    "latent_matrix_2_gpu = torch.tensor(latent_matrix_2, device=device, dtype=torch.float32)\n",
    "components_gpu = torch.tensor(svd_collab.components_, device=device, dtype=torch.float32)\n",
    "\n",
    "@lru_cache(maxsize=1000)\n",
    "def get_user_predictions(user_id, batch_size=10000):\n",
    "    # Get user's latent features\n",
    "    user_idx = user_map[user_id]\n",
    "    user_features = latent_matrix_2_gpu[user_idx]\n",
    "    \n",
    "    # Get user's rated movies for masking\n",
    "    rated_movies = set(ratings_df[ratings_df['userId'] == user_id]['movieId'])\n",
    "    \n",
    "    # Process predictions in batches\n",
    "    all_predictions = []\n",
    "    n_movies = len(movie_ids)\n",
    "    \n",
    "    for start_idx in range(0, n_movies, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, n_movies)\n",
    "        batch_components = components_gpu[:, start_idx:end_idx]\n",
    "        \n",
    "        # Calculate batch predictions\n",
    "        batch_predictions = torch.matmul(user_features, batch_components)\n",
    "        \n",
    "        # Mask rated movies in this batch\n",
    "        batch_movie_ids = movie_ids[start_idx:end_idx]\n",
    "        mask = torch.tensor([mid not in rated_movies for mid in batch_movie_ids], \n",
    "                          device=device, dtype=torch.bool)\n",
    "        \n",
    "        batch_predictions = torch.where(mask, batch_predictions,\n",
    "                                       torch.tensor(float('-inf'), device=device))\n",
    "        all_predictions.append(batch_predictions)\n",
    "    \n",
    "    return torch.cat(all_predictions)\n",
    "        \n",
    "    batch_predictions = torch.where(mask, batch_predictions, \n",
    "                                    torch.tensor(float('-inf'), device=device))\n",
    "    all_predictions.append(batch_predictions)\n",
    "    \n",
    "    return torch.cat(all_predictions)\n",
    "\n",
    "def collaborative_recommender(user_id, n_recommendations=10):\n",
    "    # Get cached predictions\n",
    "    predictions = get_user_predictions(user_id)\n",
    "    \n",
    "    # Get top recommendations\n",
    "    _, indices = torch.topk(predictions, n_recommendations)\n",
    "    top_movie_ids = [movie_ids[i] for i in indices.cpu().numpy()]\n",
    "    \n",
    "    # Get recommended movies\n",
    "    recommendations = movies_df[movies_df['movieId'].isin(top_movie_ids)]\n",
    "    return recommendations[['movieId', 'title', 'genres']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Matrix Factorization using Surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVD model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SVD Training: 100%|██████████| 1/1 [00:00<00:00,  2.75it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Training SVD model...\")\n",
    "reader = Reader(rating_scale=(0.5, 5))\n",
    "trainset = Dataset.load_from_df(ratings_df[['userId', 'movieId', 'rating']], reader).build_full_trainset()\n",
    "svd_model = SVD(n_factors=100, random_state=17)\n",
    "with tqdm(total=1, desc=\"SVD Training\") as pbar:\n",
    "    svd_model.fit(trainset)\n",
    "    pbar.update(1)\n",
    "\n",
    "# Convert SVD matrices to GPU tensors once\n",
    "svd_pu = torch.tensor(svd_model.pu, device=device, dtype=torch.float32)\n",
    "svd_qi = torch.tensor(svd_model.qi, device=device, dtype=torch.float32)\n",
    "svd_bu = torch.tensor(svd_model.bu, device=device, dtype=torch.float32)\n",
    "svd_bi = torch.tensor(svd_model.bi, device=device, dtype=torch.float32)\n",
    "svd_mu = torch.tensor([svd_model.trainset.global_mean], device=device, dtype=torch.float32)\n",
    "\n",
    "def matrix_factorization_recommender(user_id, n_recommendations=10, batch_size=10000):\n",
    "    # Get all movies that are in the trainset\n",
    "    all_movies = set(svd_model.trainset._raw2inner_id_items.keys())\n",
    "    \n",
    "    # Get user's rated movies\n",
    "    rated_movies = ratings_df[ratings_df['userId'] == user_id]['movieId'].unique()\n",
    "    \n",
    "    # Get unrated movies that are in the trainset\n",
    "    unrated_movies = np.array(list(all_movies - set(rated_movies)))\n",
    "    \n",
    "    # Get user index in SVD model\n",
    "    user_inner_id = svd_model.trainset.to_inner_uid(user_id)\n",
    "    \n",
    "    # Get user factors and bias\n",
    "    user_factors = svd_pu[user_inner_id]\n",
    "    user_bias = svd_bu[user_inner_id]\n",
    "    \n",
    "    # Process movies in batches\n",
    "    all_predictions = []\n",
    "    for i in range(0, len(unrated_movies), batch_size):\n",
    "        batch_movies = unrated_movies[i:i + batch_size]\n",
    "        \n",
    "        # Get batch factors and biases\n",
    "        movie_factors = svd_qi[batch_movies]\n",
    "        movie_biases = svd_bi[batch_movies]\n",
    "        \n",
    "        # Calculate predictions for batch\n",
    "        batch_predictions = torch.matmul(user_factors, movie_factors.T) + user_bias + movie_biases + svd_mu\n",
    "        all_predictions.append(batch_predictions)\n",
    "    \n",
    "    # Combine all predictions\n",
    "    predictions = torch.cat(all_predictions)\n",
    "    \n",
    "    # Get top recommendations\n",
    "    _, indices = torch.topk(predictions, n_recommendations)\n",
    "    recommended_movie_ids = [unrated_movies[idx] for idx in indices.cpu().numpy()]\n",
    "    \n",
    "    # Get recommended movies\n",
    "    recommended_movies = movies_df[movies_df['movieId'].isin(recommended_movie_ids)]\n",
    "    return recommended_movies[['movieId', 'title', 'genres']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hybrid Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def compute_content_scores(movie_idx, all_movie_features):\n",
    "    query_vector = get_cached_movie_features(movie_idx)\n",
    "    return torch.nn.functional.cosine_similarity(\n",
    "        query_vector.unsqueeze(0).unsqueeze(0),\n",
    "        all_movie_features.unsqueeze(0)\n",
    "    ).squeeze()\n",
    "\n",
    "async def compute_collab_scores(user_features, components_gpu):\n",
    "    return torch.matmul(user_features, components_gpu)\n",
    "\n",
    "async def compute_mf_scores(user_inner_id, user_factors, all_movie_factors, all_movie_biases):\n",
    "    return torch.matmul(user_factors, all_movie_factors.T) + svd_bu[user_inner_id] + all_movie_biases + svd_mu\n",
    "\n",
    "async def hybrid_recommender(user_id, movie_title, n_recommendations=10):\n",
    "    # Get movie index for content-based\n",
    "    movie_idx = movies_with_tags[movies_with_tags['title'] == movie_title].index[0]\n",
    "    \n",
    "    # Get user features for collaborative\n",
    "    user_idx = user_map[user_id]\n",
    "    user_features = latent_matrix_2_gpu[user_idx]\n",
    "    user_ratings = torch.tensor(user_movie_matrix[user_idx].toarray()[0], device=device, dtype=torch.float32)\n",
    "    \n",
    "    # Get all valid movie indices\n",
    "    all_movies = set(svd_model.trainset._raw2inner_id_items.keys())\n",
    "    rated_movies = ratings_df[ratings_df['userId'] == user_id]['movieId'].unique()\n",
    "    unrated_movies = np.array(list(all_movies - set(rated_movies)))\n",
    "    \n",
    "    # Map movie IDs to indices\n",
    "    movie_id_to_idx = {mid: idx for idx, mid in enumerate(movies_df['movieId'])}\n",
    "    movie_indices = np.array([movie_id_to_idx[mid] for mid in unrated_movies])\n",
    "    \n",
    "    # Get all movie features first\n",
    "    all_movie_features = latent_matrix_gpu\n",
    "    \n",
    "    # Run all predictions concurrently\n",
    "    user_inner_id = svd_model.trainset.to_inner_uid(user_id)\n",
    "    user_factors = svd_pu[user_inner_id]\n",
    "    all_movie_factors = svd_qi\n",
    "    all_movie_biases = svd_bi\n",
    "    \n",
    "    content_task = compute_content_scores(movie_idx, all_movie_features)\n",
    "    collab_task = compute_collab_scores(user_features, components_gpu)\n",
    "    mf_task = compute_mf_scores(user_inner_id, user_factors, all_movie_factors, all_movie_biases)\n",
    "    \n",
    "    # Wait for all tasks to complete\n",
    "    all_content_similarities, all_collab_predictions, all_mf_predictions = await asyncio.gather(\n",
    "        content_task, collab_task, mf_task\n",
    "    )\n",
    "    \n",
    "    # Extract scores for unrated movies only\n",
    "    content_similarities = all_content_similarities[movie_indices]\n",
    "    collab_predictions = all_collab_predictions[movie_indices]\n",
    "    mf_predictions = all_mf_predictions[movie_indices]\n",
    "    \n",
    "    # Verify tensor sizes match\n",
    "    print(f\"Sizes - Content: {content_similarities.size()}, Collab: {collab_predictions.size()}, MF: {mf_predictions.size()}\")\n",
    "    \n",
    "    # Normalize scores to [0,1] range for each set of predictions\n",
    "    content_scores = (content_similarities - content_similarities.min()) / (content_similarities.max() - content_similarities.min())\n",
    "    collab_scores = (collab_predictions - collab_predictions.min()) / (collab_predictions.max() - collab_predictions.min())\n",
    "    mf_scores = (mf_predictions - mf_predictions.min()) / (mf_predictions.max() - mf_predictions.min())\n",
    "    \n",
    "    # Combine scores with weights\n",
    "    combined_scores = content_scores * 0.3 + collab_scores * 0.4 + mf_scores * 0.3\n",
    "    \n",
    "    # Get top recommendations\n",
    "    _, top_indices = combined_scores.topk(n_recommendations)\n",
    "    top_indices = top_indices.cpu().numpy()\n",
    "    \n",
    "    # Get recommended movies\n",
    "    recommended_movie_ids = unrated_movies[top_indices]\n",
    "    recommended_movies = movies_df[movies_df['movieId'].isin(recommended_movie_ids)].copy()\n",
    "    recommended_movies['scores'] = combined_scores[top_indices].cpu().numpy()\n",
    "    recommended_movies['content_score'] = content_scores[top_indices].cpu().numpy()\n",
    "    recommended_movies['collab_score'] = collab_scores[top_indices].cpu().numpy()\n",
    "    recommended_movies['mf_score'] = mf_scores[top_indices].cpu().numpy()\n",
    "    \n",
    "    # Cache the results\n",
    "    cache.set(f\"hybrid_{user_id}_{movie_title}\", recommended_movie_ids)\n",
    "    \n",
    "    return recommended_movies.sort_values('scores', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation Metrics\n",
    "\n",
    "The following metrics show predicted ratings for recommended movies to help evaluate recommendation quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Popular Movies:\n",
      "|   movieId | title                            |   rating_mean |   rating_count | genres                                  |   predicted_rating |\n",
      "|----------:|:---------------------------------|--------------:|---------------:|:----------------------------------------|-------------------:|\n",
      "|       318 | Shawshank Redemption, The (1994) |       4.42902 |            317 | Crime|Drama                             |               5    |\n",
      "|       858 | Godfather, The (1972)            |       4.28906 |            192 | Crime|Drama                             |               5    |\n",
      "|      2959 | Fight Club (1999)                |       4.27294 |            218 | Action|Crime|Drama|Thriller             |               4.93 |\n",
      "|      1221 | Godfather: Part II, The (1974)   |       4.25969 |            129 | Crime|Drama                             |               4.97 |\n",
      "|     48516 | Departed, The (2006)             |       4.25234 |            107 | Crime|Drama|Thriller                    |               4.99 |\n",
      "|      1213 | Goodfellas (1990)                |       4.25    |            126 | Crime|Drama                             |               4.86 |\n",
      "|       912 | Casablanca (1942)                |       4.24    |            100 | Drama|Romance                           |               5    |\n",
      "|     58559 | Dark Knight, The (2008)          |       4.23826 |            149 | Action|Crime|Drama|IMAX                 |               4.98 |\n",
      "|        50 | Usual Suspects, The (1995)       |       4.23775 |            204 | Crime|Mystery|Thriller                  |               4.87 |\n",
      "|      1197 | Princess Bride, The (1987)       |       4.23239 |            142 | Action|Adventure|Comedy|Fantasy|Romance |               4.93 |\n",
      "\n",
      "Content-based Recommendations for 'Toy Story (1995)':\n",
      "|   movieId | title                              | genres                                      |   predicted_rating |\n",
      "|----------:|:-----------------------------------|:--------------------------------------------|-------------------:|\n",
      "|         1 | Toy Story (1995)                   | Adventure|Animation|Children|Comedy|Fantasy |               4.58 |\n",
      "|        14 | Nixon (1995)                       | Drama                                       |               4.62 |\n",
      "|        18 | Four Rooms (1995)                  | Comedy                                      |               4.3  |\n",
      "|        25 | Leaving Las Vegas (1995)           | Drama|Romance                               |               4.14 |\n",
      "|        11 | American President, The (1995)     | Comedy|Drama|Romance                        |               4.52 |\n",
      "|        10 | GoldenEye (1995)                   | Action|Adventure|Thriller                   |               4.05 |\n",
      "|        17 | Sense and Sensibility (1995)       | Drama|Romance                               |               4.79 |\n",
      "|        60 | Indian in the Cupboard, The (1995) | Adventure|Children|Fantasy                  |               4    |\n",
      "|         8 | Tom and Huck (1995)                | Adventure|Children                          |               3.8  |\n",
      "|         9 | Sudden Death (1995)                | Action                                      |               3.91 |\n",
      "\n",
      "Collaborative Filtering Recommendations for user 1:\n",
      "|   movieId | title                                   | genres                              |   predicted_rating |\n",
      "|----------:|:----------------------------------------|:------------------------------------|-------------------:|\n",
      "|       353 | Crow, The (1994)                        | Action|Crime|Fantasy|Thriller       |               4.41 |\n",
      "|       454 | Firm, The (1993)                        | Drama|Thriller                      |               4.41 |\n",
      "|       455 | Free Willy (1993)                       | Adventure|Children|Drama            |               3.55 |\n",
      "|      1639 | Chasing Amy (1997)                      | Comedy|Drama|Romance                |               4.38 |\n",
      "|      1663 | Stripes (1981)                          | Comedy|War                          |               4.6  |\n",
      "|      1961 | Rain Man (1988)                         | Drama                               |               4.59 |\n",
      "|      2167 | Blade (1998)                            | Action|Horror|Thriller              |               4.28 |\n",
      "|      2355 | Bug's Life, A (1998)                    | Adventure|Animation|Children|Comedy |               4.44 |\n",
      "|      4571 | Bill & Ted's Excellent Adventure (1989) | Adventure|Comedy|Sci-Fi             |               4.24 |\n",
      "|      5218 | Ice Age (2002)                          | Adventure|Animation|Children|Comedy |               4.62 |\n",
      "\n",
      "Matrix Factorization Recommendations for user 1:\n",
      "|   movieId | title                                    | genres                                   |   predicted_rating |\n",
      "|----------:|:-----------------------------------------|:-----------------------------------------|-------------------:|\n",
      "|     54771 | Invasion, The (2007)                     | Action|Drama|Horror|Sci-Fi|Thriller      |               4.36 |\n",
      "|     54787 | Death Sentence (2007)                    | Drama|Thriller                           |               4.09 |\n",
      "|     54995 | Planet Terror (2007)                     | Action|Horror|Sci-Fi                     |               4.58 |\n",
      "|     54999 | Shoot 'Em Up (2007)                      | Action|Comedy|Crime                      |               4.44 |\n",
      "|     55067 | Requiem (2006)                           | Drama|Thriller                           |               4.36 |\n",
      "|     55071 | No End in Sight (2007)                   | Documentary                              |               4.21 |\n",
      "|     55167 | Tekkonkinkreet (Tekkon kinkurîto) (2006) | Action|Adventure|Animation|Crime|Fantasy |               4.56 |\n",
      "|     55207 | Cashback (2004)                          | Comedy|Drama                             |               4.16 |\n",
      "|     55247 | Into the Wild (2007)                     | Action|Adventure|Drama                   |               4.79 |\n",
      "|     55259 | Seeker: The Dark Is Rising, The (2007)   | Action|Adventure|Drama|Fantasy           |               3.99 |\n",
      "\n",
      "Hybrid Recommendations for user 1 and 'Toy Story (1995)':\n",
      "Sizes - Content: torch.Size([9492]), Collab: torch.Size([9492]), MF: torch.Size([9492])\n",
      "|   movieId | title                                                                        | genres               |   scores |   content_score |   collab_score |   mf_score |   predicted_rating |\n",
      "|----------:|:-----------------------------------------------------------------------------|:---------------------|---------:|----------------:|---------------:|-----------:|-------------------:|\n",
      "|        16 | Casino (1995)                                                                | Crime|Drama          | 0.72408  |       0.172531  |       0.987444 |   0.924475 |               4.74 |\n",
      "|       148 | Awfully Big Adventure, An (1995)                                             | Drama                | 0.718234 |       0.27812   |       0.937204 |   0.866387 |               4.22 |\n",
      "|       155 | Beyond Rangoon (1995)                                                        | Adventure|Drama|War  | 0.715112 |       0.18159   |       0.97937  |   0.896289 |               4.01 |\n",
      "|       161 | Crimson Tide (1995)                                                          | Drama|Thriller|War   | 0.709058 |       0.0563199 |       0.980406 |   1        |               4.44 |\n",
      "|       168 | First Knight (1995)                                                          | Action|Drama|Romance | 0.699644 |       0.212449  |       0.89418  |   0.927458 |               3.96 |\n",
      "|       175 | Kids (1995)                                                                  | Drama                | 0.691627 |       0.554896  |       0.731586 |   0.77508  |               4.75 |\n",
      "|       191 | Scarlet Letter, The (1995)                                                   | Drama|Romance        | 0.683335 |       0.235596  |       0.824924 |   0.942288 |               3.64 |\n",
      "|       222 | Circle of Friends (1995)                                                     | Drama|Romance        | 0.681164 |       0.251985  |       0.860011 |   0.871879 |               4.26 |\n",
      "|       237 | Forget Paris (1995)                                                          | Comedy|Romance       | 0.681032 |       0.0563276 |       1        |   0.880445 |               3.99 |\n",
      "|       405 | Highlander III: The Sorcerer (a.k.a. Highlander: The Final Dimension) (1994) | Action|Fantasy       | 0.680515 |       1         |       0.264698 |   0.915454 |               3.77 |\n"
     ]
    }
   ],
   "source": [
    "# Set pandas display options for wide tables\n",
    "from tabulate import tabulate\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def predict_ratings(user_id, movie_ids):\n",
    "    \"\"\"Predict ratings for given user and movies using SVD model\"\"\"\n",
    "    predictions = []\n",
    "    for movie_id in movie_ids:\n",
    "        try:\n",
    "            pred = svd_model.predict(user_id, movie_id).est\n",
    "            predictions.append(round(pred, 2))\n",
    "        except:\n",
    "            predictions.append(None)\n",
    "    return predictions\n",
    "\n",
    "def display_recommendations(df, show_index=False, user_id=None):\n",
    "    \"\"\"Display recommendations with predicted ratings if user_id is provided\"\"\"\n",
    "    if user_id is not None and 'movieId' in df.columns:\n",
    "        df = df.copy()\n",
    "        df['predicted_rating'] = predict_ratings(user_id, df['movieId'])\n",
    "    print(tabulate(df, headers='keys', tablefmt='pipe', showindex=show_index))\n",
    "\n",
    "# Example usage with recommendations and predicted ratings\n",
    "user_id = 1\n",
    "movie_title = 'Toy Story (1995)'\n",
    "\n",
    "print(\"Popular Movies:\")\n",
    "display_recommendations(popularity_recommender(), user_id=user_id)\n",
    "\n",
    "print(\"\\nContent-based Recommendations for 'Toy Story (1995)':\")\n",
    "display_recommendations(content_based_recommender(movie_title), user_id=user_id)\n",
    "\n",
    "print(\"\\nCollaborative Filtering Recommendations for user 1:\")\n",
    "display_recommendations(collaborative_recommender(user_id), user_id=user_id)\n",
    "\n",
    "print(\"\\nMatrix Factorization Recommendations for user 1:\")\n",
    "display_recommendations(matrix_factorization_recommender(user_id), user_id=user_id)\n",
    "\n",
    "print(\"\\nHybrid Recommendations for user 1 and 'Toy Story (1995)':\")\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "loop = asyncio.get_event_loop()\n",
    "result = loop.run_until_complete(hybrid_recommender(user_id, movie_title))\n",
    "display_recommendations(result, user_id=user_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
