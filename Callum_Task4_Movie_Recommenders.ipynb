{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Recommender Systems\n",
    "\n",
    "This notebook implements various movie recommendation approaches:\n",
    "1. Popularity-based\n",
    "2. Content-based Filtering\n",
    "3. Collaborative Filtering\n",
    "4. Matrix Factorization\n",
    "5. Hybrid Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from surprise.model_selection import train_test_split\n",
    "import torch\n",
    "import warnings\n",
    "import asyncio\n",
    "import concurrent.futures\n",
    "from functools import lru_cache\n",
    "from typing import List, Tuple, Dict\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup device and optimizations for Apple Silicon\n",
    "device = torch.device('mps')\n",
    "torch.backends.mps.enable_fallback_kernels = True\n",
    "print(f\"Using Apple Metal device: {device}\")\n",
    "\n",
    "# Progressive Memory Management for 20M dataset\n",
    "class ProgressiveMemoryManager:\n",
    "    def __init__(self):\n",
    "        self.max_memory = torch.mps.get_memory_allocated()\n",
    "        self.active_tensors = {}\n",
    "        self.memory_threshold = 0.85  # 85% memory utilization threshold\n",
    "    \n",
    "    def get_memory_usage(self):\n",
    "        return torch.mps.get_memory_allocated() / self.max_memory\n",
    "    \n",
    "    def get_available_memory(self):\n",
    "        return self.max_memory - torch.mps.get_memory_allocated()\n",
    "    \n",
    "    def allocate_tensor(self, key, shape, dtype):\n",
    "        required_memory = np.prod(shape) * dtype.itemsize\n",
    "        \n",
    "        # Check if we need to free memory\n",
    "        if self.get_memory_usage() > self.memory_threshold:\n",
    "            self.free_inactive_tensors()\n",
    "        \n",
    "        # Allocate in smaller chunks if necessary\n",
    "        if required_memory > self.get_available_memory():\n",
    "            return self.progressive_allocation(shape, dtype)\n",
    "        \n",
    "        tensor = torch.empty(shape, dtype=dtype, device=device)\n",
    "        self.active_tensors[key] = tensor\n",
    "        return tensor\n",
    "    \n",
    "    def progressive_allocation(self, shape, dtype):\n",
    "        # Split large tensors into manageable chunks\n",
    "        chunk_size = shape[0] // 10  # Start with 10 chunks\n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(0, shape[0], chunk_size):\n",
    "            chunk_shape = list(shape)\n",
    "            chunk_shape[0] = min(chunk_size, shape[0] - i)\n",
    "            chunk = torch.empty(chunk_shape, dtype=dtype, device=device)\n",
    "            chunks.append(chunk)\n",
    "            torch.mps.synchronize()\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def free_inactive_tensors(self):\n",
    "        self.active_tensors.clear()\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "# Mini-batch processor optimized for 20M ratings\n",
    "class MiniBatchProcessor:\n",
    "    def __init__(self, initial_batch_size=500_000):\n",
    "        self.batch_size = initial_batch_size\n",
    "        self.memory_manager = ProgressiveMemoryManager()\n",
    "        self.processed_count = 0\n",
    "        self.chunk_buffer = None\n",
    "        self.max_retries = 3\n",
    "    \n",
    "    def process_ratings(self, ratings_df):\n",
    "        total_size = len(ratings_df)\n",
    "        results = []\n",
    "        retry_count = 0\n",
    "        \n",
    "        try:\n",
    "            for start_idx in range(0, total_size, self.batch_size):\n",
    "                # Dynamically adjust batch size based on available memory\n",
    "                if self.memory_manager.get_memory_usage() > 0.8:  # 80% memory threshold\n",
    "                    self.batch_size = max(100_000, self.batch_size // 2)\n",
    "                    torch.mps.empty_cache()\n",
    "                \n",
    "                end_idx = min(start_idx + self.batch_size, total_size)\n",
    "                chunk = ratings_df.iloc[start_idx:end_idx]\n",
    "                \n",
    "                try:\n",
    "                    # Process chunk with error handling\n",
    "                    chunk_result = self.process_chunk(chunk)\n",
    "                    results.append(chunk_result)\n",
    "                    \n",
    "                    # Update progress\n",
    "                    self.processed_count += len(chunk)\n",
    "                    if self.processed_count % (self.batch_size * 5) == 0:\n",
    "                        print(f\"Processed {self.processed_count:,}/{total_size:,} ratings ({(self.processed_count/total_size)*100:.1f}%)\")\n",
    "                        \n",
    "                    # Clear memory periodically\n",
    "                    if len(results) % 5 == 0:\n",
    "                        torch.mps.empty_cache()\n",
    "                        \n",
    "                except torch.cuda.OutOfMemoryError:\n",
    "                    # Handle OOM by reducing batch size and retrying\n",
    "                    if retry_count < self.max_retries:\n",
    "                        self.batch_size //= 2\n",
    "                        retry_count += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        raise\n",
    "                \n",
    "                retry_count = 0  # Reset retry counter on successful processing\n",
    "        \n",
    "        finally:\n",
    "            # Ensure memory is cleared\n",
    "            self.memory_manager.free_inactive_tensors()\n",
    "        \n",
    "        return self.merge_results(results)\n",
    "    \n",
    "    def process_chunk(self, chunk):\n",
    "        # Pre-allocate tensors if possible\n",
    "        if self.chunk_buffer is None or self.chunk_buffer[0].shape[0] != len(chunk):\n",
    "            self.chunk_buffer = (\n",
    "                torch.empty((len(chunk),), device=device, dtype=torch.int32),\n",
    "                torch.empty((len(chunk),), device=device, dtype=torch.int32),\n",
    "                torch.empty((len(chunk),), device=device, dtype=torch.float16)\n",
    "            )\n",
    "        \n",
    "        # Reuse pre-allocated tensors\n",
    "        user_ids, movie_ids, ratings = self.chunk_buffer\n",
    "        user_ids.copy_(torch.tensor(chunk['userId'].values, device=device, dtype=torch.int32))\n",
    "        movie_ids.copy_(torch.tensor(chunk['movieId'].values, device=device, dtype=torch.int32))\n",
    "        ratings.copy_(torch.tensor(chunk['rating'].values, device=device, dtype=torch.float16))\n",
    "        \n",
    "        return self.compute_chunk_embeddings(user_ids, movie_ids, ratings)\n",
    "    \n",
    "    def merge_results(self, results):\n",
    "        if not results:\n",
    "            return None\n",
    "        \n",
    "        if isinstance(results[0], torch.Tensor):\n",
    "            # Merge tensors efficiently\n",
    "            total_size = sum(r.shape[0] for r in results)\n",
    "            merged = torch.empty((total_size, *results[0].shape[1:]), \n",
    "                               dtype=results[0].dtype, \n",
    "                               device=device)\n",
    "            offset = 0\n",
    "            for r in results:\n",
    "                merged[offset:offset + r.shape[0]] = r\n",
    "                offset += r.shape[0]\n",
    "            return merged\n",
    "        else:\n",
    "            # Merge DataFrames efficiently\n",
    "            return pd.concat(results, ignore_index=True, copy=False)\n",
    "\n",
    "# Initialize global resources\n",
    "memory_manager = ProgressiveMemoryManager()\n",
    "batch_processor = MiniBatchProcessor()"
   ]
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure pandas for efficient memory usage\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Custom data types for memory efficiency\n",
    "ratings_dtypes = {\n",
    "    'userId': np.int32,\n",
    "    'movieId': np.int32,\n",
    "    'rating': np.float16,\n",
    "    'timestamp': np.int64\n",
    "}\n",
    "\n",
    "# Read datasets in chunks for large files\n",
    "def read_large_csv(filename, dtypes=None, chunksize=1_000_000):\n",
    "    chunks = []\n",
    "    for chunk in pd.read_csv(filename, dtype=dtypes, chunksize=chunksize):\n",
    "        chunks.append(chunk)\n",
    "        # Force memory cleanup\n",
    "        if len(chunks) % 10 == 0:\n",
    "            torch.mps.empty_cache()\n",
    "    return pd.concat(chunks)\n",
    "\n",
    "# Read the datasets\n",
    "movies_df = pd.read_csv('ml-latest-small/movies.csv')\n",
    "ratings_df = read_large_csv('ml-latest-small/ratings.csv', dtypes=ratings_dtypes)\n",
    "tags_df = pd.read_csv('ml-latest-small/tags.csv')\n",
    "\n",
    "# Process tags in batches\n",
    "def process_tags_in_batches(tags_df, batch_size=10_000):\n",
    "    grouped_tags = []\n",
    "    for start in range(0, len(tags_df), batch_size):\n",
    "        batch = tags_df.iloc[start:start + batch_size]\n",
    "        batch_grouped = batch.groupby('movieId')['tag'].apply(lambda x: ' '.join(x))\n",
    "        grouped_tags.append(batch_grouped)\n",
    "        torch.mps.empty_cache()\n",
    "    return pd.concat(grouped_tags).reset_index()\n",
    "\n",
    "# Merge tags efficiently\n",
    "tags_grouped = process_tags_in_batches(tags_df)\n",
    "movies_with_tags = pd.merge(movies_df, tags_grouped, on='movieId', how='left')\n",
    "movies_with_tags['tag'] = movies_with_tags['tag'].fillna('')\n",
    "\n",
    "# Clear memory\n",
    "del tags_df, tags_grouped\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Popularity-based Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def popularity_recommender(n_recommendations=10):\n",
    "    # Process ratings in batches for memory efficiency\n",
    "    batch_size = get_optimal_batch_size(len(ratings_df))\n",
    "    stats_list = []\n",
    "    \n",
    "    for start in range(0, len(ratings_df), batch_size):\n",
    "        batch = ratings_df.iloc[start:start + batch_size]\n",
    "        batch_stats = batch.groupby('movieId').agg({\n",
    "            'rating': ['count', 'mean']\n",
    "        }).reset_index()\n",
    "        stats_list.append(batch_stats)\n",
    "        \n",
    "        # Clear memory periodically\n",
    "        if start % (batch_size * 10) == 0:\n",
    "            torch.mps.empty_cache()\n",
    "    \n",
    "    # Combine batch results efficiently\n",
    "    movie_stats = pd.concat(stats_list).groupby('movieId').agg({\n",
    "        ('rating', 'count'): 'sum',\n",
    "        ('rating', 'mean'): 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Rename columns\n",
    "    movie_stats.columns = ['movieId', 'rating_count', 'rating_mean']\n",
    "    \n",
    "    # Use GPU for sorting if dataset is large\n",
    "    if len(movie_stats) > 10000:\n",
    "        # Convert to GPU tensors\n",
    "        counts = torch.tensor(movie_stats['rating_count'].values, device=device)\n",
    "        means = torch.tensor(movie_stats['rating_mean'].values, device=device)\n",
    "        \n",
    "        # Filter and sort on GPU\n",
    "        mask = counts >= 100\n",
    "        filtered_indices = torch.nonzero(mask).squeeze()\n",
    "        \n",
    "        # Calculate weighted scores\n",
    "        scores = means[filtered_indices] * torch.log1p(counts[filtered_indices])\n",
    "        _, top_indices = torch.topk(scores, min(n_recommendations * 2, len(scores)))\n",
    "        \n",
    "        # Get final indices and convert back to CPU\n",
    "        selected_indices = filtered_indices[top_indices].cpu().numpy()\n",
    "        popular_movies = movie_stats.iloc[selected_indices]\n",
    "    else:\n",
    "        # Use pandas for smaller datasets\n",
    "        popular_movies = movie_stats[movie_stats['rating_count'] >= 100]\n",
    "        popular_movies['score'] = popular_movies['rating_mean'] * np.log1p(popular_movies['rating_count'])\n",
    "        popular_movies = popular_movies.nlargest(n_recommendations * 2, 'score')\n",
    "    \n",
    "    # Get movie details\n",
    "    recommendations = pd.merge(popular_movies, movies_df, on='movieId')\n",
    "    \n",
    "    # Cache the results\n",
    "    cache_key = f\"popular_{n_recommendations}\"\n",
    "    result = recommendations[['title', 'rating_mean', 'rating_count']].head(n_recommendations)\n",
    "    cache.set(cache_key, result)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Content-based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF vectorizer and convert to sparse GPU tensor\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(movies_with_tags['tag'])\n",
    "\n",
    "# Convert sparse matrix to GPU tensor efficiently\n",
    "indices = torch.tensor([tfidf_matrix.nonzero()[0], tfidf_matrix.nonzero()[1]], device=device)\n",
    "values = torch.tensor(tfidf_matrix.data, device=device, dtype=torch.float16)  # Use half precision\n",
    "tfidf_matrix_gpu = torch.sparse_coo_tensor(\n",
    "    indices=indices,\n",
    "    values=values,\n",
    "    size=tfidf_matrix.shape,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Perform SVD on GPU using torch operations\n",
    "U, S, V = torch.svd(tfidf_matrix_gpu.to_dense())  # Convert to dense for SVD\n",
    "latent_matrix_gpu = (U[:, :100] @ torch.diag(S[:100])).to(dtype=torch.float16)  # Use half precision\n",
    "\n",
    "# Cache for frequently accessed movie features\n",
    "@lru_cache(maxsize=1000)\n",
    "def get_cached_movie_features(movie_idx):\n",
    "    return latent_matrix_gpu[movie_idx]\n",
    "\n",
    "def content_based_recommender(movie_title, n_recommendations=10):\n",
    "    # Get movie index and features from cache\n",
    "    movie_idx = movies_with_tags[movies_with_tags['title'] == movie_title].index[0]\n",
    "    query_vector = get_cached_movie_features(movie_idx)\n",
    "    \n",
    "    # Batch compute similarities using optimized operations\n",
    "    similarities = torch.nn.functional.cosine_similarity(\n",
    "        query_vector.unsqueeze(0).unsqueeze(0),\n",
    "        latent_matrix_gpu.unsqueeze(0)\n",
    "    ).squeeze()\n",
    "    \n",
    "    # Get top recommendations using MPS-optimized topk\n",
    "    _, similar_indices = similarities.topk(n_recommendations + 1)\n",
    "    similar_indices = similar_indices[1:].cpu().numpy()\n",
    "    \n",
    "    # Cache the results\n",
    "    cache.set(f\"content_{movie_title}\", similar_indices)\n",
    "    \n",
    "    return movies_with_tags.iloc[similar_indices][['title', 'genres']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process user-movie matrix in batches\n",
    "def create_user_movie_matrix(ratings_df, batch_size=10_000):\n",
    "    unique_users = ratings_df['userId'].unique()\n",
    "    unique_movies = ratings_df['movieId'].unique()\n",
    "    \n",
    "    # Initialize sparse matrix on GPU\n",
    "    values = []\n",
    "    row_indices = []\n",
    "    col_indices = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for start in range(0, len(ratings_df), batch_size):\n",
    "        batch = ratings_df.iloc[start:start + batch_size]\n",
    "        \n",
    "        # Get user and movie indices\n",
    "        user_idx = batch['userId'].map(lambda x: np.where(unique_users == x)[0][0])\n",
    "        movie_idx = batch['movieId'].map(lambda x: np.where(unique_movies == x)[0][0])\n",
    "        \n",
    "        values.extend(batch['rating'].values)\n",
    "        row_indices.extend(user_idx)\n",
    "        col_indices.extend(movie_idx)\n",
    "        \n",
    "        # Clear GPU cache periodically\n",
    "        if start % (batch_size * 10) == 0:\n",
    "            torch.mps.empty_cache()\n",
    "    \n",
    "    # Create sparse tensor\n",
    "    indices = torch.tensor([row_indices, col_indices], device=device)\n",
    "    values = torch.tensor(values, device=device, dtype=torch.float16)\n",
    "    \n",
    "    return torch.sparse_coo_tensor(\n",
    "        indices=indices,\n",
    "        values=values,\n",
    "        size=(len(unique_users), len(unique_movies)),\n",
    "        device=device\n",
    "    ).to_dense()\n",
    "\n",
    "# Create and process user-movie matrix\n",
    "user_movie_matrix = create_user_movie_matrix(ratings_df)\n",
    "\n",
    "# Perform SVD using PyTorch for GPU acceleration\n",
    "U, S, V = torch.svd(user_movie_matrix)\n",
    "latent_matrix_2_gpu = (U[:, :100] @ torch.diag(S[:100])).to(dtype=torch.float16)\n",
    "components_gpu = V[:, :100].T.to(dtype=torch.float16)\n",
    "\n",
    "def collaborative_recommender(user_id, n_recommendations=10):\n",
    "    # Get cached user features if available\n",
    "    cache_key = f\"collab_user_{user_id}\"\n",
    "    cached_result = cache.get(cache_key)\n",
    "    if cached_result is not None:\n",
    "        return cached_result\n",
    "    \n",
    "    # Get user's latent features\n",
    "    user_idx = np.where(ratings_df['userId'].unique() == user_id)[0][0]\n",
    "    user_features = latent_matrix_2_gpu[user_idx]\n",
    "    \n",
    "    # Get user ratings efficiently\n",
    "    user_ratings = user_movie_matrix[user_idx]\n",
    "    \n",
    "    # Calculate predictions using optimized operations\n",
    "    predicted_ratings = torch.matmul(\n",
    "        user_features.to(dtype=torch.float16),\n",
    "        components_gpu\n",
    "    )\n",
    "    \n",
    "    # Use efficient masking\n",
    "    unrated_mask = (user_ratings == 0)\n",
    "    predictions = predicted_ratings.clone()\n",
    "    predictions[~unrated_mask] = float('-inf')\n",
    "    \n",
    "    # Get top recommendations efficiently\n",
    "    _, indices = torch.topk(predictions, min(n_recommendations * 2, len(predictions)))\n",
    "    indices = indices.cpu().numpy()\n",
    "    \n",
    "    # Filter and sort recommendations\n",
    "    movie_ids = ratings_df['movieId'].unique()[indices]\n",
    "    recommended_movies = movies_df[movies_df['movieId'].isin(movie_ids)].copy()\n",
    "    recommended_movies['score'] = predictions[indices].cpu().numpy()\n",
    "    recommended_movies = recommended_movies.nlargest(n_recommendations, 'score')\n",
    "    \n",
    "    # Cache the results\n",
    "    result = recommended_movies[['title', 'genres']]\n",
    "    cache.set(cache_key, result)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Matrix Factorization using Surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for Surprise with efficient memory usage\n",
    "reader = Reader(rating_scale=(0.5, 5))\n",
    "data = Dataset.load_from_df(ratings_df[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "# Split data\n",
    "trainset, testset = train_test_split(data, test_size=0.25, random_state=42)\n",
    "\n",
    "# Train SVD model\n",
    "svd_model = SVD(n_factors=100, random_state=42)\n",
    "svd_model.fit(trainset)\n",
    "\n",
    "# Convert SVD matrices to GPU tensors with half precision\n",
    "svd_pu = torch.tensor(svd_model.pu, device=device, dtype=torch.float16)\n",
    "svd_qi = torch.tensor(svd_model.qi, device=device, dtype=torch.float16)\n",
    "svd_bu = torch.tensor(svd_model.bu, device=device, dtype=torch.float16)\n",
    "svd_bi = torch.tensor(svd_model.bi, device=device, dtype=torch.float16)\n",
    "svd_mu = torch.tensor([svd_model.trainset.global_mean], device=device, dtype=torch.float16)\n",
    "\n",
    "# Pre-compute movie indices mapping for faster lookup\n",
    "movie_id_to_inner = {mid: svd_model.trainset.to_inner_iid(mid) for mid in movies_df['movieId'].unique()}\n",
    "\n",
    "def matrix_factorization_recommender(user_id, n_recommendations=10):\n",
    "    # Check cache first\n",
    "    cache_key = f\"mf_user_{user_id}\"\n",
    "    cached_result = cache.get(cache_key)\n",
    "    if cached_result is not None:\n",
    "        return cached_result\n",
    "    \n",
    "    # Get user's rated movies efficiently using sets\n",
    "    rated_movies = set(ratings_df[ratings_df['userId'] == user_id]['movieId'])\n",
    "    all_movies = set(movies_df['movieId'])\n",
    "    unrated_movies = list(all_movies - rated_movies)\n",
    "    \n",
    "    # Get user index and convert to GPU tensor\n",
    "    user_inner_id = svd_model.trainset.to_inner_uid(user_id)\n",
    "    user_factors = svd_pu[user_inner_id]\n",
    "    user_bias = svd_bu[user_inner_id]\n",
    "    \n",
    "    # Process predictions in batches\n",
    "    batch_size = get_optimal_batch_size(len(unrated_movies))\n",
    "    all_predictions = []\n",
    "    movie_ids = []\n",
    "    \n",
    "    for i in range(0, len(unrated_movies), batch_size):\n",
    "        batch_movies = unrated_movies[i:i + batch_size]\n",
    "        batch_inner_ids = torch.tensor([movie_id_to_inner[mid] for mid in batch_movies], device=device)\n",
    "        \n",
    "        # Get movie factors and biases for batch\n",
    "        batch_factors = svd_qi[batch_inner_ids]\n",
    "        batch_biases = svd_bi[batch_inner_ids]\n",
    "        \n",
    "        # Calculate predictions for batch\n",
    "        batch_preds = torch.matmul(user_factors, batch_factors.T) + user_bias + batch_biases + svd_mu\n",
    "        \n",
    "        all_predictions.append(batch_preds)\n",
    "        movie_ids.extend(batch_movies)\n",
    "        \n",
    "        # Clear GPU cache periodically\n",
    "        if i % (batch_size * 10) == 0:\n",
    "            torch.mps.empty_cache()\n",
    "    \n",
    "    # Combine predictions and get top recommendations\n",
    "    predictions = torch.cat(all_predictions)\n",
    "    _, indices = torch.topk(predictions, min(n_recommendations, len(predictions)))\n",
    "    recommended_movie_ids = [movie_ids[idx] for idx in indices.cpu().numpy()]\n",
    "    \n",
    "    # Get recommended movies\n",
    "    result = movies_df[movies_df['movieId'].isin(recommended_movie_ids)][['title', 'genres']]\n",
    "    \n",
    "    # Cache the results\n",
    "    cache.set(cache_key, result)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hybrid Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def hybrid_recommender(user_id, movie_title, n_recommendations=10):\n",
    "    # Get movie index for content-based\n",
    "    movie_idx = movies_with_tags[movies_with_tags['title'] == movie_title].index[0]\n",
    "    query_vector = get_cached_movie_features(movie_idx)\n",
    "    \n",
    "    # Get user features for collaborative\n",
    "    user_idx = user_movie_matrix.index.get_loc(user_id)\n",
    "    user_features = latent_matrix_2_gpu[user_idx]\n",
    "    user_ratings = torch.tensor(user_movie_matrix.loc[user_id].values, device=device, dtype=torch.float16)\n",
    "    \n",
    "    # Batch compute all similarities and predictions on GPU\n",
    "    content_similarities = torch.nn.functional.cosine_similarity(\n",
    "        query_vector.unsqueeze(0).unsqueeze(0),\n",
    "        latent_matrix_gpu.unsqueeze(0)\n",
    "    ).squeeze()\n",
    "    \n",
    "    collab_predictions = torch.matmul(user_features, components_gpu)\n",
    "    unrated_mask = (user_ratings == 0)\n",
    "    collab_predictions[~unrated_mask] = float('-inf')\n",
    "    \n",
    "    # Get user index in SVD model for matrix factorization\n",
    "    user_inner_id = svd_model.trainset.to_inner_uid(user_id)\n",
    "    user_factors = svd_pu[user_inner_id]\n",
    "    movie_factors = svd_qi\n",
    "    movie_biases = svd_bi\n",
    "    mf_predictions = torch.matmul(user_factors, movie_factors.T) + svd_bu[user_inner_id] + movie_biases + svd_mu\n",
    "    \n",
    "    # Normalize scores to [0,1] range\n",
    "    content_scores = (content_similarities - content_similarities.min()) / (content_similarities.max() - content_similarities.min())\n",
    "    collab_scores = (collab_predictions - collab_predictions[unrated_mask].min()) / (collab_predictions[unrated_mask].max() - collab_predictions[unrated_mask].min())\n",
    "    mf_scores = (mf_predictions - mf_predictions.min()) / (mf_predictions.max() - mf_predictions.min())\n",
    "    \n",
    "    # Combine scores with weights\n",
    "    combined_scores = (content_scores * 0.3 + collab_scores * 0.4 + mf_scores * 0.3)\n",
    "    \n",
    "    # Get top recommendations\n",
    "    _, indices = combined_scores.topk(n_recommendations)\n",
    "    indices = indices.cpu().numpy()\n",
    "    \n",
    "    # Get recommended movies\n",
    "    recommended_movies = movies_df.iloc[indices][['title', 'genres']]\n",
    "    recommended_movies['scores'] = combined_scores[indices].cpu().numpy()\n",
    "    recommended_movies['content_score'] = content_scores[indices].cpu().numpy()\n",
    "    recommended_movies['collab_score'] = collab_scores[indices].cpu().numpy()\n",
    "    recommended_movies['mf_score'] = mf_scores[indices].cpu().numpy()\n",
    "    \n",
    "    # Cache the results\n",
    "    cache.set(f\"hybrid_{user_id}_{movie_title}\", indices)\n",
    "    \n",
    "    return recommended_movies.sort_values('scores', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "print(\"Popular Movies:\")\n",
    "print(popularity_recommender())\n",
    "\n",
    "print(\"\\nContent-based Recommendations for 'Toy Story (1995)':\")\n",
    "print(content_based_recommender('Toy Story (1995)'))\n",
    "\n",
    "print(\"\\nCollaborative Filtering Recommendations for user 1:\")\n",
    "print(collaborative_recommender(1))\n",
    "\n",
    "print(\"\\nMatrix Factorization Recommendations for user 1:\")\n",
    "print(matrix_factorization_recommender(1))\n",
    "\n",
    "print(\"\\nHybrid Recommendations for user 1 and 'Toy Story (1995)':\")\n",
    "print(hybrid_recommender(1, 'Toy Story (1995)'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
