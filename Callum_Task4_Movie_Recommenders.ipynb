{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Recommender Systems\n",
    "\n",
    "This notebook implements various movie recommendation approaches:\n",
    "1. Popularity-based\n",
    "2. Content-based Filtering\n",
    "3. Collaborative Filtering\n",
    "4. Matrix Factorization\n",
    "5. Hybrid Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from surprise import Dataset, Reader, SVD\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import asyncio\n",
    "from functools import lru_cache\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from scipy import sparse\n",
    "import h5py\n",
    "import joblib\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Metal device: mps\n"
     ]
    }
   ],
   "source": [
    "# Setup device and optimizations for Apple Silicon\n",
    "device = torch.device('mps')\n",
    "torch.backends.mps.enable_fallback_kernels = True\n",
    "print(f\"Using Apple Metal device: {device}\")\n",
    "\n",
    "# Advanced caching mechanism\n",
    "class RecommendationCache:\n",
    "    def __init__(self, max_size=1000):\n",
    "        self.cache = {}\n",
    "        self.max_size = max_size\n",
    "    \n",
    "    @lru_cache(maxsize=1000)\n",
    "    def get_movie_features(self, movie_idx):\n",
    "        return self.latent_matrix_gpu[movie_idx]\n",
    "    \n",
    "    def get(self, key):\n",
    "        return self.cache.get(key)\n",
    "    \n",
    "    def set(self, key, value):\n",
    "        if len(self.cache) >= self.max_size:\n",
    "            self.cache.pop(next(iter(self.cache)))\n",
    "        self.cache[key] = value\n",
    "\n",
    "cache = RecommendationCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading movies...\n",
      "Loading ratings...\n",
      "Loading tags...\n",
      "Processing tags...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate chunk size based on available memory\n",
    "def get_optimal_chunk_size():\n",
    "    available_mem = psutil.virtual_memory().available\n",
    "    # Use 20% of available memory for chunk size\n",
    "    return int((available_mem * 0.2) / (8 * 1024))\n",
    "\n",
    "# Read datasets in chunks with disk caching\n",
    "def read_chunked_csv(filename, chunksize=None):\n",
    "    cache_file = Path(f\".cache_{Path(filename).stem}.joblib\")\n",
    "    if cache_file.exists():\n",
    "        return joblib.load(cache_file)\n",
    "    \n",
    "    chunksize = chunksize or get_optimal_chunk_size()\n",
    "    chunks = []\n",
    "    total_rows = sum(1 for _ in open(filename)) - 1  # Subtract header\n",
    "    \n",
    "    with tqdm(total=total_rows, desc=f\"Loading {filename}\") as pbar:\n",
    "        for chunk in pd.read_csv(filename, chunksize=chunksize):\n",
    "            chunks.append(chunk)\n",
    "            pbar.update(len(chunk))\n",
    "    \n",
    "    result = pd.concat(chunks)\n",
    "    joblib.dump(result, cache_file)\n",
    "    return result\n",
    "\n",
    "print(\"Loading movies...\")\n",
    "movies_df = pd.read_csv('ml-latest-small/movies.csv')\n",
    "\n",
    "print(\"Loading ratings...\")\n",
    "ratings_df = read_chunked_csv('ml-latest-small/ratings.csv')\n",
    "\n",
    "print(\"Loading tags...\")\n",
    "tags_df = read_chunked_csv('ml-latest-small/tags.csv')\n",
    "\n",
    "# Process tags in chunks\n",
    "print(\"Processing tags...\")\n",
    "tags_grouped = tags_df.groupby('movieId')['tag'].apply(\n",
    "    lambda x: ' '.join(x[:1000] if len(x) > 1000 else x)\n",
    ").reset_index()\n",
    "\n",
    "# Merge and clean up\n",
    "movies_with_tags = pd.merge(movies_df, tags_grouped, on='movieId', how='left')\n",
    "movies_with_tags['tag'] = movies_with_tags['tag'].fillna('')\n",
    "\n",
    "# Clean up memory\n",
    "del tags_df, tags_grouped\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Popularity-based Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def popularity_recommender(n_recommendations=10):\n",
    "    # Calculate mean rating and number of ratings for each movie\n",
    "    movie_stats = ratings_df.groupby('movieId').agg({\n",
    "        'rating': ['count', 'mean']\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Rename columns\n",
    "    movie_stats.columns = ['movieId', 'rating_count', 'rating_mean']\n",
    "    \n",
    "    # Filter movies with minimum number of ratings (e.g., 100)\n",
    "    popular_movies = movie_stats[movie_stats['rating_count'] >= 100]\n",
    "    \n",
    "    # Sort by rating mean and count\n",
    "    popular_movies = popular_movies.sort_values(['rating_mean', 'rating_count'], ascending=[False, False])\n",
    "    \n",
    "    # Get movie titles\n",
    "    recommendations = pd.merge(popular_movies, movies_df, on='movieId')\n",
    "    \n",
    "    return recommendations[['movieId', 'title', 'rating_mean', 'rating_count', 'genres']].head(n_recommendations) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Content-based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing TF-IDF in batches...\n"
     ]
    }
   ],
   "source": [
    "def process_tfidf_in_batches(texts, batch_size=1000):\n",
    "    cache_file = Path(\".cache_tfidf.h5\")\n",
    "    if cache_file.exists():\n",
    "        with h5py.File(cache_file, 'r') as f:\n",
    "            return torch.tensor(f['latent_matrix'][()], device=device)\n",
    "    \n",
    "    print(\"Creating TF-IDF vectors...\")\n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "    \n",
    "    # First pass to fit vocabulary\n",
    "    print(\"Fitting TF-IDF vocabulary...\")\n",
    "    tfidf.fit(texts)\n",
    "    \n",
    "    # Process in batches\n",
    "    n_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "    latent_matrices = []\n",
    "    \n",
    "    for i in tqdm(range(n_batches), desc=\"Processing TF-IDF batches\"):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(texts))\n",
    "        batch_texts = texts[start_idx:end_idx]\n",
    "        \n",
    "        # Transform batch and keep sparse\n",
    "        batch_tfidf = tfidf.transform(batch_texts)\n",
    "        \n",
    "        # Compute SVD for batch with explicit float32 dtype for MPS compatibility\n",
    "        batch_U, batch_S, batch_V = torch.svd(\n",
    "            torch.tensor(batch_tfidf.toarray(), device=device, dtype=torch.float32)\n",
    "        )\n",
    "        batch_latent = (batch_U[:, :100] @ torch.diag(batch_S[:100]))\n",
    "        latent_matrices.append(batch_latent.cpu().numpy())\n",
    "    \n",
    "    # Combine results\n",
    "    latent_matrix = np.vstack(latent_matrices)\n",
    "    \n",
    "    # Cache results\n",
    "    with h5py.File(cache_file, 'w') as f:\n",
    "        f.create_dataset('latent_matrix', data=latent_matrix)\n",
    "    \n",
    "    return torch.tensor(latent_matrix, device=device)\n",
    "\n",
    "print(\"Processing TF-IDF in batches...\")\n",
    "latent_matrix_gpu = process_tfidf_in_batches(movies_with_tags['tag'])\n",
    "\n",
    "# Cache for frequently accessed movie features\n",
    "@lru_cache(maxsize=1000)\n",
    "def get_cached_movie_features(movie_idx):\n",
    "    return latent_matrix_gpu[movie_idx]\n",
    "\n",
    "def content_based_recommender(movie_title, n_recommendations=10):\n",
    "    # Get movie index and features from cache\n",
    "    movie_idx = movies_with_tags[movies_with_tags['title'] == movie_title].index[0]\n",
    "    query_vector = get_cached_movie_features(movie_idx)\n",
    "    \n",
    "    # Batch compute similarities using optimized operations\n",
    "    similarities = torch.nn.functional.cosine_similarity(\n",
    "        query_vector.unsqueeze(0).unsqueeze(0),\n",
    "        latent_matrix_gpu.unsqueeze(0)\n",
    "    ).squeeze()\n",
    "    \n",
    "    # Get top recommendations using MPS-optimized topk\n",
    "    _, similar_indices = similarities.topk(n_recommendations + 1)\n",
    "    similar_indices = similar_indices[1:].cpu().numpy()\n",
    "    \n",
    "    # Cache the results\n",
    "    cache.set(f\"content_{movie_title}\", similar_indices)\n",
    "    \n",
    "    recommendations = movies_with_tags.iloc[similar_indices][['movieId', 'title', 'genres']]                                                                      \n",
    "    recommendations['title'] = recommendations['title'].str.ljust(50)                                                                                   \n",
    "    return recommendations  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating user-movie matrix...\n",
      "Creating sparse user-movie matrix...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[119]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m matrix, user_map, movie_map\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCreating user-movie matrix...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m user_movie_matrix, user_map, movie_map = create_sparse_matrix(ratings_df)\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPerforming collaborative filtering SVD...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total=\u001b[32m1\u001b[39m, desc=\u001b[33m\"\u001b[39m\u001b[33mSVD Progress\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "def create_sparse_matrix(ratings_df, batch_size=100000):\n",
    "    print(\"Creating sparse user-movie matrix...\")\n",
    "    cache_file = Path(\".cache_collab.npz\")\n",
    "    if cache_file.exists():\n",
    "        return sparse.load_npz(cache_file)\n",
    "    \n",
    "    rows, cols, data = [], [], []\n",
    "    user_map = {}\n",
    "    movie_map = {}\n",
    "    \n",
    "    # Process in batches\n",
    "    total_rows = len(ratings_df)\n",
    "    with tqdm(total=total_rows, desc=\"Building sparse matrix\") as pbar:\n",
    "        for start in range(0, total_rows, batch_size):\n",
    "            batch = ratings_df.iloc[start:start + batch_size]\n",
    "            \n",
    "            for _, row in batch.iterrows():\n",
    "                if row['userId'] not in user_map:\n",
    "                    user_map[row['userId']] = len(user_map)\n",
    "                if row['movieId'] not in movie_map:\n",
    "                    movie_map[row['movieId']] = len(movie_map)\n",
    "                \n",
    "                rows.append(user_map[row['userId']])\n",
    "                cols.append(movie_map[row['movieId']])\n",
    "                data.append(row['rating'])\n",
    "                \n",
    "            pbar.update(len(batch))\n",
    "    \n",
    "    matrix = sparse.csr_matrix((data, (rows, cols)), \n",
    "                              shape=(len(user_map), len(movie_map)))\n",
    "    \n",
    "    # Cache the results\n",
    "    sparse.save_npz(cache_file, matrix)\n",
    "    joblib.dump((user_map, movie_map), \".cache_collab_maps.joblib\")\n",
    "    \n",
    "    return matrix, user_map, movie_map\n",
    "\n",
    "print(\"Creating user-movie matrix...\")\n",
    "user_movie_matrix, user_map, movie_map = create_sparse_matrix(ratings_df)\n",
    "\n",
    "print(\"Performing collaborative filtering SVD...\")\n",
    "with tqdm(total=1, desc=\"SVD Progress\") as pbar:\n",
    "    svd_collab = TruncatedSVD(n_components=100)\n",
    "    latent_matrix_2 = svd_collab.fit_transform(user_movie_matrix)\n",
    "    pbar.update(1)\n",
    "\n",
    "# Convert matrices to GPU tensors once\n",
    "latent_matrix_2_gpu = torch.tensor(latent_matrix_2, device=device, dtype=torch.float32)\n",
    "components_gpu = torch.tensor(svd_collab.components_, device=device, dtype=torch.float32)\n",
    "\n",
    "@lru_cache(maxsize=1000)\n",
    "def get_user_predictions(user_id, batch_size=10000):\n",
    "    # Get user's latent features\n",
    "    user_idx = user_map[user_id]\n",
    "    user_features = latent_matrix_2_gpu[user_idx]\n",
    "    \n",
    "    # Get user's rated movies for masking\n",
    "    rated_movies = set(ratings_df[ratings_df['userId'] == user_id]['movieId'])\n",
    "    \n",
    "    # Process predictions in batches\n",
    "    all_predictions = []\n",
    "    n_movies = len(movie_ids)\n",
    "    \n",
    "    for start_idx in range(0, n_movies, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, n_movies)\n",
    "        batch_components = components_gpu[:, start_idx:end_idx]\n",
    "        \n",
    "        # Calculate batch predictions\n",
    "        batch_predictions = torch.matmul(user_features, batch_components)\n",
    "        \n",
    "        # Mask rated movies in this batch\n",
    "        batch_movie_ids = movie_ids[start_idx:end_idx]\n",
    "        mask = torch.tensor([mid not in rated_movies for mid in batch_movie_ids], \n",
    "                          device=device, dtype=torch.bool)\n",
    "        \n",
    "        batch_predictions = torch.where(mask, batch_predictions, \n",
    "                                       torch.tensor(float('-inf'), device=device))\n",
    "        all_predictions.append(batch_predictions)\n",
    "    \n",
    "    return torch.cat(all_predictions)\n",
    "\n",
    "def collaborative_recommender(user_id, n_recommendations=10):\n",
    "    # Get cached predictions\n",
    "    predictions = get_user_predictions(user_id)\n",
    "    \n",
    "    # Get top recommendations\n",
    "    _, indices = torch.topk(predictions, n_recommendations)\n",
    "    top_movie_ids = [movie_ids[i] for i in indices.cpu().numpy()]\n",
    "    \n",
    "    # Get recommended movies\n",
    "    recommendations = movies_df[movies_df['movieId'].isin(top_movie_ids)]\n",
    "    return recommendations[['movieId', 'title', 'genres']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Matrix Factorization using Surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVD model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SVD Training: 100%|██████████| 1/1 [00:00<00:00,  3.22it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Training SVD model...\")\n",
    "reader = Reader(rating_scale=(0.5, 5))\n",
    "trainset = Dataset.load_from_df(ratings_df[['userId', 'movieId', 'rating']], reader).build_full_trainset()\n",
    "svd_model = SVD(n_factors=100, random_state=17)\n",
    "with tqdm(total=1, desc=\"SVD Training\") as pbar:\n",
    "    svd_model.fit(trainset)\n",
    "    pbar.update(1)\n",
    "\n",
    "# Convert SVD matrices to GPU tensors once\n",
    "svd_pu = torch.tensor(svd_model.pu, device=device, dtype=torch.float32)\n",
    "svd_qi = torch.tensor(svd_model.qi, device=device, dtype=torch.float32)\n",
    "svd_bu = torch.tensor(svd_model.bu, device=device, dtype=torch.float32)\n",
    "svd_bi = torch.tensor(svd_model.bi, device=device, dtype=torch.float32)\n",
    "svd_mu = torch.tensor([svd_model.trainset.global_mean], device=device, dtype=torch.float32)\n",
    "\n",
    "def matrix_factorization_recommender(user_id, n_recommendations=10, batch_size=10000):\n",
    "    # Get all movies that are in the trainset\n",
    "    all_movies = set(svd_model.trainset._raw2inner_id_items.keys())\n",
    "    \n",
    "    # Get user's rated movies\n",
    "    rated_movies = ratings_df[ratings_df['userId'] == user_id]['movieId'].unique()\n",
    "    \n",
    "    # Get unrated movies that are in the trainset\n",
    "    unrated_movies = np.array(list(all_movies - set(rated_movies)))\n",
    "    \n",
    "    # Get user index in SVD model\n",
    "    user_inner_id = svd_model.trainset.to_inner_uid(user_id)\n",
    "    \n",
    "    # Get user factors and bias\n",
    "    user_factors = svd_pu[user_inner_id]\n",
    "    user_bias = svd_bu[user_inner_id]\n",
    "    \n",
    "    # Process movies in batches\n",
    "    all_predictions = []\n",
    "    for i in range(0, len(unrated_movies), batch_size):\n",
    "        batch_movies = unrated_movies[i:i + batch_size]\n",
    "        \n",
    "        # Get batch factors and biases\n",
    "        movie_factors = svd_qi[batch_movies]\n",
    "        movie_biases = svd_bi[batch_movies]\n",
    "        \n",
    "        # Calculate predictions for batch\n",
    "        batch_predictions = torch.matmul(user_factors, movie_factors.T) + user_bias + movie_biases + svd_mu\n",
    "        all_predictions.append(batch_predictions)\n",
    "    \n",
    "    # Combine all predictions\n",
    "    predictions = torch.cat(all_predictions)\n",
    "    \n",
    "    # Get top recommendations\n",
    "    _, indices = torch.topk(predictions, n_recommendations)\n",
    "    recommended_movie_ids = [unrated_movies[idx] for idx in indices.cpu().numpy()]\n",
    "    \n",
    "    # Get recommended movies\n",
    "    recommended_movies = movies_df[movies_df['movieId'].isin(recommended_movie_ids)]\n",
    "    return recommended_movies[['movieId', 'title', 'genres']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hybrid Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def compute_content_scores(movie_idx, all_movie_features):\n",
    "    query_vector = get_cached_movie_features(movie_idx)\n",
    "    return torch.nn.functional.cosine_similarity(\n",
    "        query_vector.unsqueeze(0).unsqueeze(0),\n",
    "        all_movie_features.unsqueeze(0)\n",
    "    ).squeeze()\n",
    "\n",
    "async def compute_collab_scores(user_features, components_gpu):\n",
    "    return torch.matmul(user_features, components_gpu)\n",
    "\n",
    "async def compute_mf_scores(user_inner_id, user_factors, all_movie_factors, all_movie_biases):\n",
    "    return torch.matmul(user_factors, all_movie_factors.T) + svd_bu[user_inner_id] + all_movie_biases + svd_mu\n",
    "\n",
    "async def hybrid_recommender(user_id, movie_title, n_recommendations=10):\n",
    "    # Get movie index for content-based\n",
    "    movie_idx = movies_with_tags[movies_with_tags['title'] == movie_title].index[0]\n",
    "    \n",
    "    # Get user features for collaborative\n",
    "    user_idx = user_movie_matrix.index.get_loc(user_id)\n",
    "    user_features = latent_matrix_2_gpu[user_idx]\n",
    "    user_ratings = torch.tensor(user_movie_matrix.loc[user_id].values, device=device, dtype=torch.float32)\n",
    "    \n",
    "    # Get all valid movie indices\n",
    "    all_movies = set(svd_model.trainset._raw2inner_id_items.keys())\n",
    "    rated_movies = ratings_df[ratings_df['userId'] == user_id]['movieId'].unique()\n",
    "    unrated_movies = np.array(list(all_movies - set(rated_movies)))\n",
    "    \n",
    "    # Map movie IDs to indices\n",
    "    movie_id_to_idx = {mid: idx for idx, mid in enumerate(movies_df['movieId'])}\n",
    "    movie_indices = np.array([movie_id_to_idx[mid] for mid in unrated_movies])\n",
    "    \n",
    "    # Get all movie features first\n",
    "    all_movie_features = latent_matrix_gpu\n",
    "    \n",
    "    # Run all predictions concurrently\n",
    "    user_inner_id = svd_model.trainset.to_inner_uid(user_id)\n",
    "    user_factors = svd_pu[user_inner_id]\n",
    "    all_movie_factors = svd_qi\n",
    "    all_movie_biases = svd_bi\n",
    "    \n",
    "    content_task = compute_content_scores(movie_idx, all_movie_features)\n",
    "    collab_task = compute_collab_scores(user_features, components_gpu)\n",
    "    mf_task = compute_mf_scores(user_inner_id, user_factors, all_movie_factors, all_movie_biases)\n",
    "    \n",
    "    # Wait for all tasks to complete\n",
    "    all_content_similarities, all_collab_predictions, all_mf_predictions = await asyncio.gather(\n",
    "        content_task, collab_task, mf_task\n",
    "    )\n",
    "    \n",
    "    # Extract scores for unrated movies only\n",
    "    content_similarities = all_content_similarities[movie_indices]\n",
    "    collab_predictions = all_collab_predictions[movie_indices]\n",
    "    mf_predictions = all_mf_predictions[movie_indices]\n",
    "    \n",
    "    # Verify tensor sizes match\n",
    "    print(f\"Sizes - Content: {content_similarities.size()}, Collab: {collab_predictions.size()}, MF: {mf_predictions.size()}\")\n",
    "    \n",
    "    # Normalize scores to [0,1] range for each set of predictions\n",
    "    content_scores = (content_similarities - content_similarities.min()) / (content_similarities.max() - content_similarities.min())\n",
    "    collab_scores = (collab_predictions - collab_predictions.min()) / (collab_predictions.max() - collab_predictions.min())\n",
    "    mf_scores = (mf_predictions - mf_predictions.min()) / (mf_predictions.max() - mf_predictions.min())\n",
    "    \n",
    "    # Combine scores with weights\n",
    "    combined_scores = content_scores * 0.3 + collab_scores * 0.4 + mf_scores * 0.3\n",
    "    \n",
    "    # Get top recommendations\n",
    "    _, top_indices = combined_scores.topk(n_recommendations)\n",
    "    top_indices = top_indices.cpu().numpy()\n",
    "    \n",
    "    # Get recommended movies\n",
    "    recommended_movie_ids = unrated_movies[top_indices]\n",
    "    recommended_movies = movies_df[movies_df['movieId'].isin(recommended_movie_ids)].copy()\n",
    "    recommended_movies['scores'] = combined_scores[top_indices].cpu().numpy()\n",
    "    recommended_movies['content_score'] = content_scores[top_indices].cpu().numpy()\n",
    "    recommended_movies['collab_score'] = collab_scores[top_indices].cpu().numpy()\n",
    "    recommended_movies['mf_score'] = mf_scores[top_indices].cpu().numpy()\n",
    "    \n",
    "    # Cache the results\n",
    "    cache.set(f\"hybrid_{user_id}_{movie_title}\", recommended_movie_ids)\n",
    "    \n",
    "    return recommended_movies.sort_values('scores', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation Metrics\n",
    "\n",
    "The following metrics show predicted ratings for recommended movies to help evaluate recommendation quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Popular Movies:\n",
      "|   movieId | title                            |   rating_mean |   rating_count | genres                                  |   predicted_rating |\n",
      "|----------:|:---------------------------------|--------------:|---------------:|:----------------------------------------|-------------------:|\n",
      "|       318 | Shawshank Redemption, The (1994) |       4.42902 |            317 | Crime|Drama                             |               5    |\n",
      "|       858 | Godfather, The (1972)            |       4.28906 |            192 | Crime|Drama                             |               5    |\n",
      "|      2959 | Fight Club (1999)                |       4.27294 |            218 | Action|Crime|Drama|Thriller             |               4.93 |\n",
      "|      1221 | Godfather: Part II, The (1974)   |       4.25969 |            129 | Crime|Drama                             |               4.97 |\n",
      "|     48516 | Departed, The (2006)             |       4.25234 |            107 | Crime|Drama|Thriller                    |               4.99 |\n",
      "|      1213 | Goodfellas (1990)                |       4.25    |            126 | Crime|Drama                             |               4.86 |\n",
      "|       912 | Casablanca (1942)                |       4.24    |            100 | Drama|Romance                           |               5    |\n",
      "|     58559 | Dark Knight, The (2008)          |       4.23826 |            149 | Action|Crime|Drama|IMAX                 |               4.98 |\n",
      "|        50 | Usual Suspects, The (1995)       |       4.23775 |            204 | Crime|Mystery|Thriller                  |               4.87 |\n",
      "|      1197 | Princess Bride, The (1987)       |       4.23239 |            142 | Action|Adventure|Comedy|Fantasy|Romance |               4.93 |\n",
      "\n",
      "Content-based Recommendations for 'Toy Story (1995)':\n",
      "|   movieId | title                              | genres                                      |   predicted_rating |\n",
      "|----------:|:-----------------------------------|:--------------------------------------------|-------------------:|\n",
      "|         1 | Toy Story (1995)                   | Adventure|Animation|Children|Comedy|Fantasy |               4.58 |\n",
      "|        14 | Nixon (1995)                       | Drama                                       |               4.62 |\n",
      "|        18 | Four Rooms (1995)                  | Comedy                                      |               4.3  |\n",
      "|        25 | Leaving Las Vegas (1995)           | Drama|Romance                               |               4.14 |\n",
      "|        11 | American President, The (1995)     | Comedy|Drama|Romance                        |               4.52 |\n",
      "|        10 | GoldenEye (1995)                   | Action|Adventure|Thriller                   |               4.05 |\n",
      "|        17 | Sense and Sensibility (1995)       | Drama|Romance                               |               4.79 |\n",
      "|        60 | Indian in the Cupboard, The (1995) | Adventure|Children|Fantasy                  |               4    |\n",
      "|         8 | Tom and Huck (1995)                | Adventure|Children                          |               3.8  |\n",
      "|         9 | Sudden Death (1995)                | Action                                      |               3.91 |\n",
      "\n",
      "Collaborative Filtering Recommendations for user 1:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'csr_matrix' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[89]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m display_recommendations(content_based_recommender(movie_title), user_id=user_id)\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCollaborative Filtering Recommendations for user 1:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m display_recommendations(\u001b[43mcollaborative_recommender\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m)\u001b[49m, user_id=user_id)\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMatrix Factorization Recommendations for user 1:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m display_recommendations(matrix_factorization_recommender(user_id), user_id=user_id)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[86]\u001b[39m\u001b[32m, line 84\u001b[39m, in \u001b[36mcollaborative_recommender\u001b[39m\u001b[34m(user_id, n_recommendations)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcollaborative_recommender\u001b[39m(user_id, n_recommendations=\u001b[32m10\u001b[39m):\n\u001b[32m     83\u001b[39m     \u001b[38;5;66;03m# Get cached predictions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     predictions = \u001b[43mget_user_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m     \u001b[38;5;66;03m# Get top recommendations\u001b[39;00m\n\u001b[32m     87\u001b[39m     _, indices = torch.topk(predictions, n_recommendations)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[86]\u001b[39m\u001b[32m, line 72\u001b[39m, in \u001b[36mget_user_predictions\u001b[39m\u001b[34m(user_id, batch_size)\u001b[39m\n\u001b[32m     69\u001b[39m batch_predictions = torch.matmul(user_features, batch_components)\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Mask rated movies in this batch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m batch_movie_ids = \u001b[43muser_movie_matrix\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m[start_idx:end_idx]\n\u001b[32m     73\u001b[39m mask = torch.tensor([mid \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m rated_movies \u001b[38;5;28;01mfor\u001b[39;00m mid \u001b[38;5;129;01min\u001b[39;00m batch_movie_ids], \n\u001b[32m     74\u001b[39m                   device=device, dtype=torch.bool)\n\u001b[32m     76\u001b[39m batch_predictions = torch.where(mask, batch_predictions, \n\u001b[32m     77\u001b[39m                                torch.tensor(\u001b[38;5;28mfloat\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m-inf\u001b[39m\u001b[33m'\u001b[39m), device=device))\n",
      "\u001b[31mAttributeError\u001b[39m: 'csr_matrix' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "# Set pandas display options for wide tables\n",
    "from tabulate import tabulate\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def predict_ratings(user_id, movie_ids):\n",
    "    \"\"\"Predict ratings for given user and movies using SVD model\"\"\"\n",
    "    predictions = []\n",
    "    for movie_id in movie_ids:\n",
    "        try:\n",
    "            pred = svd_model.predict(user_id, movie_id).est\n",
    "            predictions.append(round(pred, 2))\n",
    "        except:\n",
    "            predictions.append(None)\n",
    "    return predictions\n",
    "\n",
    "def display_recommendations(df, show_index=False, user_id=None):\n",
    "    \"\"\"Display recommendations with predicted ratings if user_id is provided\"\"\"\n",
    "    if user_id is not None and 'movieId' in df.columns:\n",
    "        df = df.copy()\n",
    "        df['predicted_rating'] = predict_ratings(user_id, df['movieId'])\n",
    "    print(tabulate(df, headers='keys', tablefmt='pipe', showindex=show_index))\n",
    "\n",
    "# Example usage with recommendations and predicted ratings\n",
    "user_id = 1\n",
    "movie_title = 'Toy Story (1995)'\n",
    "\n",
    "print(\"Popular Movies:\")\n",
    "display_recommendations(popularity_recommender(), user_id=user_id)\n",
    "\n",
    "print(\"\\nContent-based Recommendations for 'Toy Story (1995)':\")\n",
    "display_recommendations(content_based_recommender(movie_title), user_id=user_id)\n",
    "\n",
    "print(\"\\nCollaborative Filtering Recommendations for user 1:\")\n",
    "display_recommendations(collaborative_recommender(user_id), user_id=user_id)\n",
    "\n",
    "print(\"\\nMatrix Factorization Recommendations for user 1:\")\n",
    "display_recommendations(matrix_factorization_recommender(user_id), user_id=user_id)\n",
    "\n",
    "print(\"\\nHybrid Recommendations for user 1 and 'Toy Story (1995)':\")\n",
    "result = asyncio.run(hybrid_recommender(user_id, movie_title))\n",
    "display_recommendations(result, user_id=user_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
