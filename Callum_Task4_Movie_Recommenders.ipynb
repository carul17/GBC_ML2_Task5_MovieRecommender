{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Recommender Systems\n",
    "\n",
    "This notebook implements various movie recommendation approaches:\n",
    "1. Popularity-based\n",
    "2. Content-based Filtering\n",
    "3. Collaborative Filtering\n",
    "4. Matrix Factorization\n",
    "5. Hybrid Approach\n",
    "\n",
    "Source: https://grouplens.org/datasets/movielens/20m/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from surprise import Dataset, Reader, SVD\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "from functools import lru_cache\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from scipy import sparse\n",
    "import h5py\n",
    "import joblib\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Metal device: mps\n"
     ]
    }
   ],
   "source": [
    "# Setup device and optimizations for Apple Silicon\n",
    "device = torch.device('mps')\n",
    "torch.backends.mps.enable_fallback_kernels = True\n",
    "print(f\"Using Apple Metal device: {device}\")\n",
    "\n",
    "# Advanced caching mechanism\n",
    "class RecommendationCache:\n",
    "    def __init__(self, max_size=1000):\n",
    "        self.cache = {}\n",
    "        self.max_size = max_size\n",
    "    \n",
    "    @lru_cache(maxsize=1000)\n",
    "    def get_movie_features(self, movie_idx):\n",
    "        return self.latent_matrix_gpu[movie_idx]\n",
    "    \n",
    "    def get(self, key):\n",
    "        return self.cache.get(key)\n",
    "    \n",
    "    def set(self, key, value):\n",
    "        if len(self.cache) >= self.max_size:\n",
    "            self.cache.pop(next(iter(self.cache)))\n",
    "        self.cache[key] = value\n",
    "\n",
    "cache = RecommendationCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading movies...\n",
      "Loading ratings...\n",
      "Loading tags...\n",
      "Processing tags...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "890"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate chunk size based on available memory\n",
    "def get_optimal_chunk_size():\n",
    "    available_mem = psutil.virtual_memory().available\n",
    "    # Use 20% of available memory for chunk size\n",
    "    return int((available_mem * 0.2) / (8 * 1024))\n",
    "\n",
    "# Read datasets in chunks with disk caching\n",
    "def read_chunked_csv(filename, chunksize=None):\n",
    "    cache_file = Path(f\".cache_{Path(filename).stem}.joblib\")\n",
    "    if cache_file.exists():\n",
    "        return joblib.load(cache_file)\n",
    "    \n",
    "    chunksize = chunksize or get_optimal_chunk_size()\n",
    "    chunks = []\n",
    "    total_rows = sum(1 for _ in open(filename)) - 1  # Subtract header\n",
    "    \n",
    "    with tqdm(total=total_rows, desc=f\"Loading {filename}\") as pbar:\n",
    "        for chunk in pd.read_csv(filename, chunksize=chunksize):\n",
    "            chunks.append(chunk)\n",
    "            pbar.update(len(chunk))\n",
    "    \n",
    "    result = pd.concat(chunks)\n",
    "    joblib.dump(result, cache_file)\n",
    "    return result\n",
    "\n",
    "print(\"Loading movies...\")\n",
    "movies_df = pd.read_csv('ml-20m/movies.csv')\n",
    "\n",
    "print(\"Loading ratings...\")\n",
    "ratings_df = read_chunked_csv('ml-20m/ratings.csv')\n",
    "\n",
    "print(\"Loading tags...\")\n",
    "tags_df = read_chunked_csv('ml-20m/tags.csv')\n",
    "\n",
    "# Process tags in chunks\n",
    "print(\"Processing tags...\")\n",
    "tags_df['tag'] = tags_df['tag'].fillna('').astype(str)\n",
    "tags_grouped = tags_df.groupby('movieId')['tag'].apply(\n",
    "    lambda x: ' '.join(x[:1000] if len(x) > 1000 else x)\n",
    ").reset_index()\n",
    "\n",
    "# Merge and clean up\n",
    "movies_with_tags = pd.merge(movies_df, tags_grouped, on='movieId', how='left')\n",
    "movies_with_tags['tag'] = movies_with_tags['tag'].fillna('')\n",
    "\n",
    "# Clean up memory\n",
    "del tags_df, tags_grouped\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Popularity-based Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def popularity_recommender(n_recommendations=10):\n",
    "    # Calculate mean rating and number of ratings for each movie\n",
    "    movie_stats = ratings_df.groupby('movieId').agg({\n",
    "        'rating': ['count', 'mean']\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Rename columns\n",
    "    movie_stats.columns = ['movieId', 'rating_count', 'rating_mean']\n",
    "    \n",
    "    # Filter movies with minimum number of ratings (e.g., 100)\n",
    "    popular_movies = movie_stats[movie_stats['rating_count'] >= 100]\n",
    "    \n",
    "    # Sort by rating mean and count\n",
    "    popular_movies = popular_movies.sort_values(['rating_mean', 'rating_count'], ascending=[False, False])\n",
    "    \n",
    "    # Get movie titles\n",
    "    recommendations = pd.merge(popular_movies, movies_df, on='movieId')\n",
    "    \n",
    "    return recommendations[['movieId', 'title', 'rating_mean', 'rating_count', 'genres']].head(n_recommendations) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Content-based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing TF-IDF in batches...\n",
      "Creating TF-IDF vectors...\n",
      "Fitting TF-IDF vocabulary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing TF-IDF batches: 100%|██████████| 28/28 [00:25<00:00,  1.10it/s]\n"
     ]
    }
   ],
   "source": [
    "def process_tfidf_in_batches(texts, batch_size=1000):\n",
    "    cache_file = Path(\".cache_tfidf.h5\")\n",
    "    if cache_file.exists():\n",
    "        with h5py.File(cache_file, 'r') as f:\n",
    "            return torch.tensor(f['latent_matrix'][()], device=device)\n",
    "    \n",
    "    print(\"Creating TF-IDF vectors...\")\n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "    \n",
    "    # First pass to fit vocabulary\n",
    "    print(\"Fitting TF-IDF vocabulary...\")\n",
    "    tfidf.fit(texts)\n",
    "    \n",
    "    # Process in batches\n",
    "    n_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "    latent_matrices = []\n",
    "    \n",
    "    for i in tqdm(range(n_batches), desc=\"Processing TF-IDF batches\"):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(texts))\n",
    "        batch_texts = texts[start_idx:end_idx]\n",
    "        \n",
    "        # Transform batch and keep sparse\n",
    "        batch_tfidf = tfidf.transform(batch_texts)\n",
    "        \n",
    "        # Compute SVD for batch with explicit float32 dtype for MPS compatibility\n",
    "        batch_U, batch_S, batch_V = torch.svd(\n",
    "            torch.tensor(batch_tfidf.toarray(), device=device, dtype=torch.float32)\n",
    "        )\n",
    "        batch_latent = (batch_U[:, :100] @ torch.diag(batch_S[:100]))\n",
    "        latent_matrices.append(batch_latent.cpu().numpy())\n",
    "    \n",
    "    # Combine results\n",
    "    latent_matrix = np.vstack(latent_matrices)\n",
    "    \n",
    "    # Cache results\n",
    "    with h5py.File(cache_file, 'w') as f:\n",
    "        f.create_dataset('latent_matrix', data=latent_matrix)\n",
    "    \n",
    "    return torch.tensor(latent_matrix, device=device)\n",
    "\n",
    "print(\"Processing TF-IDF in batches...\")\n",
    "latent_matrix_gpu = process_tfidf_in_batches(movies_with_tags['tag'])\n",
    "\n",
    "# Cache for frequently accessed movie features\n",
    "@lru_cache(maxsize=1000)\n",
    "def get_cached_movie_features(movie_idx):\n",
    "    return latent_matrix_gpu[movie_idx]\n",
    "\n",
    "def content_based_recommender(movie_title, n_recommendations=10):\n",
    "    # Get movie index and features from cache\n",
    "    movie_idx = movies_with_tags[movies_with_tags['title'] == movie_title].index[0]\n",
    "    query_vector = get_cached_movie_features(movie_idx)\n",
    "    \n",
    "    # Batch compute similarities using optimized operations\n",
    "    similarities = torch.nn.functional.cosine_similarity(\n",
    "        query_vector.unsqueeze(0).unsqueeze(0),\n",
    "        latent_matrix_gpu.unsqueeze(0)\n",
    "    ).squeeze()\n",
    "    \n",
    "    # Get top recommendations using MPS-optimized topk\n",
    "    _, similar_indices = similarities.topk(n_recommendations + 1)\n",
    "    similar_indices = similar_indices[1:].cpu().numpy()\n",
    "    \n",
    "    # Cache the results\n",
    "    cache.set(f\"content_{movie_title}\", similar_indices)\n",
    "    \n",
    "    recommendations = movies_with_tags.iloc[similar_indices][['movieId', 'title', 'genres']]                                                                      \n",
    "    recommendations['title'] = recommendations['title'].str.ljust(50)                                                                                   \n",
    "    return recommendations  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating user-movie matrix...\n",
      "Creating sparse user-movie matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building sparse matrix: 100%|██████████| 20000263/20000263 [02:56<00:00, 113195.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing collaborative filtering SVD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SVD Progress: 100%|██████████| 1/1 [00:06<00:00,  6.15s/it]\n"
     ]
    }
   ],
   "source": [
    "def create_sparse_matrix(ratings_df, batch_size=100000):\n",
    "    print(\"Creating sparse user-movie matrix...\")\n",
    "    cache_file = Path(\".cache_collab.npz\")\n",
    "    cache_maps = Path(\".cache_collab_maps.joblib\")\n",
    "    \n",
    "    if cache_file.exists() and cache_maps.exists():\n",
    "        matrix = sparse.load_npz(cache_file)\n",
    "        cache_data = joblib.load(cache_maps)\n",
    "        if len(cache_data) == 2:  # Old cache format\n",
    "            user_map, movie_map = cache_data\n",
    "            movie_ids = list(movie_map.keys())\n",
    "        else:  # New cache format\n",
    "            user_map, movie_map, movie_ids = cache_data\n",
    "        return matrix, user_map, movie_map, movie_ids\n",
    "    \n",
    "    rows, cols, data = [], [], []\n",
    "    user_map = {}\n",
    "    movie_map = {}\n",
    "    movie_ids = []\n",
    "    \n",
    "    # Process in batches\n",
    "    total_rows = len(ratings_df)\n",
    "    with tqdm(total=total_rows, desc=\"Building sparse matrix\") as pbar:\n",
    "        for start in range(0, total_rows, batch_size):\n",
    "            batch = ratings_df.iloc[start:start + batch_size]\n",
    "            \n",
    "            for _, row in batch.iterrows():\n",
    "                if row['userId'] not in user_map:\n",
    "                    user_map[row['userId']] = len(user_map)\n",
    "                if row['movieId'] not in movie_map:\n",
    "                    movie_map[row['movieId']] = len(movie_map)\n",
    "                    movie_ids.append(row['movieId'])\n",
    "                \n",
    "                rows.append(user_map[row['userId']])\n",
    "                cols.append(movie_map[row['movieId']])\n",
    "                data.append(row['rating'])\n",
    "                \n",
    "            pbar.update(len(batch))\n",
    "    \n",
    "    matrix = sparse.csr_matrix((data, (rows, cols)), \n",
    "                              shape=(len(user_map), len(movie_map)))\n",
    "    \n",
    "    # Cache the results\n",
    "    sparse.save_npz(cache_file, matrix)\n",
    "    joblib.dump((user_map, movie_map, movie_ids), cache_maps)\n",
    "    \n",
    "    return matrix, user_map, movie_map, movie_ids\n",
    "\n",
    "print(\"Creating user-movie matrix...\")\n",
    "user_movie_matrix, user_map, movie_map, movie_ids = create_sparse_matrix(ratings_df)\n",
    "\n",
    "print(\"Performing collaborative filtering SVD...\")\n",
    "with tqdm(total=1, desc=\"SVD Progress\") as pbar:\n",
    "    svd_collab = TruncatedSVD(n_components=100)\n",
    "    latent_matrix_2 = svd_collab.fit_transform(user_movie_matrix)\n",
    "    pbar.update(1)\n",
    "\n",
    "# Convert matrices to GPU tensors once\n",
    "latent_matrix_2_gpu = torch.tensor(latent_matrix_2, device=device, dtype=torch.float32)\n",
    "components_gpu = torch.tensor(svd_collab.components_, device=device, dtype=torch.float32)\n",
    "\n",
    "@lru_cache(maxsize=1000)\n",
    "def get_user_predictions(user_id, batch_size=10000):\n",
    "    # Get user's latent features\n",
    "    user_idx = user_map[user_id]\n",
    "    user_features = latent_matrix_2_gpu[user_idx]\n",
    "    \n",
    "    # Get user's rated movies for masking\n",
    "    rated_movies = set(ratings_df[ratings_df['userId'] == user_id]['movieId'])\n",
    "    \n",
    "    # Process predictions in batches\n",
    "    all_predictions = []\n",
    "    n_movies = len(movie_ids)\n",
    "    \n",
    "    for start_idx in range(0, n_movies, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, n_movies)\n",
    "        batch_components = components_gpu[:, start_idx:end_idx]\n",
    "        \n",
    "        # Calculate batch predictions\n",
    "        batch_predictions = torch.matmul(user_features, batch_components)\n",
    "        \n",
    "        # Mask rated movies in this batch\n",
    "        batch_movie_ids = movie_ids[start_idx:end_idx]\n",
    "        mask = torch.tensor([mid not in rated_movies for mid in batch_movie_ids], \n",
    "                          device=device, dtype=torch.bool)\n",
    "        \n",
    "        batch_predictions = torch.where(mask, batch_predictions,\n",
    "                                       torch.tensor(float('-inf'), device=device))\n",
    "        all_predictions.append(batch_predictions)\n",
    "    \n",
    "    return torch.cat(all_predictions)\n",
    "        \n",
    "    batch_predictions = torch.where(mask, batch_predictions, \n",
    "                                    torch.tensor(float('-inf'), device=device))\n",
    "    all_predictions.append(batch_predictions)\n",
    "    \n",
    "    return torch.cat(all_predictions)\n",
    "\n",
    "def collaborative_recommender(user_id, n_recommendations=10):\n",
    "    # Get cached predictions\n",
    "    predictions = get_user_predictions(user_id)\n",
    "    \n",
    "    # Get top recommendations\n",
    "    _, indices = torch.topk(predictions, n_recommendations)\n",
    "    top_movie_ids = [movie_ids[i] for i in indices.cpu().numpy()]\n",
    "    \n",
    "    # Get recommended movies\n",
    "    recommendations = movies_df[movies_df['movieId'].isin(top_movie_ids)]\n",
    "    return recommendations[['movieId', 'title', 'genres']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Matrix Factorization using Surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVD model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SVD Training: 100%|██████████| 1/1 [00:52<00:00, 52.91s/it]\n"
     ]
    }
   ],
   "source": [
    "print(\"Training SVD model...\")\n",
    "reader = Reader(rating_scale=(0.5, 5))\n",
    "trainset = Dataset.load_from_df(ratings_df[['userId', 'movieId', 'rating']], reader).build_full_trainset()\n",
    "svd_model = SVD(n_factors=100, random_state=17)\n",
    "with tqdm(total=1, desc=\"SVD Training\") as pbar:\n",
    "    svd_model.fit(trainset)\n",
    "    pbar.update(1)\n",
    "\n",
    "# Convert SVD matrices to GPU tensors once\n",
    "svd_pu = torch.tensor(svd_model.pu, device=device, dtype=torch.float32)\n",
    "svd_qi = torch.tensor(svd_model.qi, device=device, dtype=torch.float32)\n",
    "svd_bu = torch.tensor(svd_model.bu, device=device, dtype=torch.float32)\n",
    "svd_bi = torch.tensor(svd_model.bi, device=device, dtype=torch.float32)\n",
    "svd_mu = torch.tensor([svd_model.trainset.global_mean], device=device, dtype=torch.float32)\n",
    "\n",
    "def matrix_factorization_recommender(user_id, n_recommendations=10, batch_size=10000):\n",
    "    # Get all movies that are in the trainset\n",
    "    all_movies = set(svd_model.trainset._raw2inner_id_items.keys())\n",
    "    \n",
    "    # Get user's rated movies\n",
    "    rated_movies = ratings_df[ratings_df['userId'] == user_id]['movieId'].unique()\n",
    "    \n",
    "    # Get unrated movies that are in the trainset\n",
    "    unrated_movies = np.array(list(all_movies - set(rated_movies)))\n",
    "    \n",
    "    # Get user index in SVD model\n",
    "    user_inner_id = svd_model.trainset.to_inner_uid(user_id)\n",
    "    \n",
    "    # Get user factors and bias\n",
    "    user_factors = svd_pu[user_inner_id]\n",
    "    user_bias = svd_bu[user_inner_id]\n",
    "    \n",
    "    # Process movies in batches\n",
    "    all_predictions = []\n",
    "    for i in range(0, len(unrated_movies), batch_size):\n",
    "        batch_movies = unrated_movies[i:i + batch_size]\n",
    "        \n",
    "        # Get batch factors and biases\n",
    "        movie_factors = svd_qi[batch_movies]\n",
    "        movie_biases = svd_bi[batch_movies]\n",
    "        \n",
    "        # Calculate predictions for batch\n",
    "        batch_predictions = torch.matmul(user_factors, movie_factors.T) + user_bias + movie_biases + svd_mu\n",
    "        all_predictions.append(batch_predictions)\n",
    "    \n",
    "    # Combine all predictions\n",
    "    predictions = torch.cat(all_predictions)\n",
    "    \n",
    "    # Get top recommendations\n",
    "    _, indices = torch.topk(predictions, n_recommendations)\n",
    "    recommended_movie_ids = [unrated_movies[idx] for idx in indices.cpu().numpy()]\n",
    "    \n",
    "    # Get recommended movies\n",
    "    recommended_movies = movies_df[movies_df['movieId'].isin(recommended_movie_ids)]\n",
    "    return recommended_movies[['movieId', 'title', 'genres']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hybrid Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def compute_content_scores(movie_idx, all_movie_features):\n",
    "    query_vector = get_cached_movie_features(movie_idx)\n",
    "    return torch.nn.functional.cosine_similarity(\n",
    "        query_vector.unsqueeze(0).unsqueeze(0),\n",
    "        all_movie_features.unsqueeze(0)\n",
    "    ).squeeze()\n",
    "\n",
    "async def compute_collab_scores(user_features, components_gpu):\n",
    "    return torch.matmul(user_features, components_gpu)\n",
    "\n",
    "async def compute_mf_scores(user_inner_id, user_factors, all_movie_factors, all_movie_biases):\n",
    "    return torch.matmul(user_factors, all_movie_factors.T) + svd_bu[user_inner_id] + all_movie_biases + svd_mu\n",
    "\n",
    "async def hybrid_recommender(user_id, movie_title, n_recommendations=10):\n",
    "    # Get movie index for content-based\n",
    "    movie_idx = movies_with_tags[movies_with_tags['title'] == movie_title].index[0]\n",
    "    \n",
    "    # Get user features for collaborative\n",
    "    user_idx = user_map[user_id]\n",
    "    user_features = latent_matrix_2_gpu[user_idx]\n",
    "    user_ratings = torch.tensor(user_movie_matrix[user_idx].toarray()[0], device=device, dtype=torch.float32)\n",
    "    \n",
    "    # Get all valid movie indices\n",
    "    all_movies = set(svd_model.trainset._raw2inner_id_items.keys())\n",
    "    rated_movies = ratings_df[ratings_df['userId'] == user_id]['movieId'].unique()\n",
    "    unrated_movies = np.array(list(all_movies - set(rated_movies)))\n",
    "    \n",
    "    # Map movie IDs to indices\n",
    "    movie_id_to_idx = {mid: idx for idx, mid in enumerate(movies_df['movieId'])}\n",
    "    movie_indices = np.array([movie_id_to_idx[mid] for mid in unrated_movies])\n",
    "    \n",
    "    # Get all movie features first\n",
    "    all_movie_features = latent_matrix_gpu\n",
    "    \n",
    "    # Run all predictions concurrently\n",
    "    user_inner_id = svd_model.trainset.to_inner_uid(user_id)\n",
    "    user_factors = svd_pu[user_inner_id]\n",
    "    all_movie_factors = svd_qi\n",
    "    all_movie_biases = svd_bi\n",
    "    \n",
    "    content_task = compute_content_scores(movie_idx, all_movie_features)\n",
    "    collab_task = compute_collab_scores(user_features, components_gpu)\n",
    "    mf_task = compute_mf_scores(user_inner_id, user_factors, all_movie_factors, all_movie_biases)\n",
    "    \n",
    "    # Wait for all tasks to complete\n",
    "    all_content_similarities, all_collab_predictions, all_mf_predictions = await asyncio.gather(\n",
    "        content_task, collab_task, mf_task\n",
    "    )\n",
    "    \n",
    "    # Extract scores for unrated movies only\n",
    "    content_similarities = all_content_similarities[movie_indices]\n",
    "    collab_predictions = all_collab_predictions[movie_indices]\n",
    "    mf_predictions = all_mf_predictions[movie_indices]\n",
    "    \n",
    "    # Verify tensor sizes match\n",
    "    print(f\"Sizes - Content: {content_similarities.size()}, Collab: {collab_predictions.size()}, MF: {mf_predictions.size()}\")\n",
    "    \n",
    "    # Normalize scores to [0,1] range for each set of predictions\n",
    "    content_scores = (content_similarities - content_similarities.min()) / (content_similarities.max() - content_similarities.min())\n",
    "    collab_scores = (collab_predictions - collab_predictions.min()) / (collab_predictions.max() - collab_predictions.min())\n",
    "    mf_scores = (mf_predictions - mf_predictions.min()) / (mf_predictions.max() - mf_predictions.min())\n",
    "    \n",
    "    # Combine scores with weights\n",
    "    combined_scores = content_scores * 0.3 + collab_scores * 0.4 + mf_scores * 0.3\n",
    "    \n",
    "    # Get top recommendations\n",
    "    _, top_indices = combined_scores.topk(n_recommendations)\n",
    "    top_indices = top_indices.cpu().numpy()\n",
    "    \n",
    "    # Get recommended movies\n",
    "    recommended_movie_ids = unrated_movies[top_indices]\n",
    "    recommended_movies = movies_df[movies_df['movieId'].isin(recommended_movie_ids)].copy()\n",
    "    recommended_movies['scores'] = combined_scores[top_indices].cpu().numpy()\n",
    "    recommended_movies['content_score'] = content_scores[top_indices].cpu().numpy()\n",
    "    recommended_movies['collab_score'] = collab_scores[top_indices].cpu().numpy()\n",
    "    recommended_movies['mf_score'] = mf_scores[top_indices].cpu().numpy()\n",
    "    \n",
    "    # Cache the results\n",
    "    cache.set(f\"hybrid_{user_id}_{movie_title}\", recommended_movie_ids)\n",
    "    \n",
    "    return recommended_movies.sort_values('scores', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation Metrics\n",
    "\n",
    "The following metrics show predicted ratings for recommended movies to help evaluate recommendation quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Popular Movies:\n",
      "|   movieId | title                                         |   rating_mean |   rating_count | genres                  |   predicted_rating |\n",
      "|----------:|:----------------------------------------------|--------------:|---------------:|:------------------------|-------------------:|\n",
      "|       318 | Shawshank Redemption, The (1994)              |       4.44699 |          63366 | Crime|Drama             |               4.35 |\n",
      "|       858 | Godfather, The (1972)                         |       4.36473 |          41355 | Crime|Drama             |               3.86 |\n",
      "|        50 | Usual Suspects, The (1995)                    |       4.33437 |          47006 | Crime|Mystery|Thriller  |               4.05 |\n",
      "|       527 | Schindler's List (1993)                       |       4.31018 |          50054 | Drama|War               |               4.09 |\n",
      "|      1221 | Godfather: Part II, The (1974)                |       4.27564 |          27398 | Crime|Drama             |               3.82 |\n",
      "|      2019 | Seven Samurai (Shichinin no samurai) (1954)   |       4.27418 |          11611 | Action|Adventure|Drama  |               3.81 |\n",
      "|       904 | Rear Window (1954)                            |       4.27133 |          17449 | Mystery|Thriller        |               3.91 |\n",
      "|      7502 | Band of Brothers (2001)                       |       4.26318 |           4305 | Action|Drama|War        |               4.07 |\n",
      "|       912 | Casablanca (1942)                             |       4.25833 |          24349 | Drama|Romance           |               3.83 |\n",
      "|       922 | Sunset Blvd. (a.k.a. Sunset Boulevard) (1950) |       4.25693 |           6525 | Drama|Film-Noir|Romance |               3.7  |\n",
      "\n",
      "Content-based Recommendations for 'Toy Story (1995)':\n",
      "|   movieId | title                          | genres                             |   predicted_rating |\n",
      "|----------:|:-------------------------------|:-----------------------------------|-------------------:|\n",
      "|         2 | Jumanji (1995)                 | Adventure|Children|Fantasy         |               3.92 |\n",
      "|        17 | Sense and Sensibility (1995)   | Drama|Romance                      |               3.81 |\n",
      "|        21 | Get Shorty (1995)              | Comedy|Crime|Thriller              |               3.35 |\n",
      "|        15 | Cutthroat Island (1995)        | Action|Adventure|Romance           |               3.53 |\n",
      "|        20 | Money Train (1995)             | Action|Comedy|Crime|Drama|Thriller |               3.39 |\n",
      "|        57 | Home for the Holidays (1995)   | Drama                              |               3.62 |\n",
      "|        11 | American President, The (1995) | Comedy|Drama|Romance               |               3.81 |\n",
      "|        50 | Usual Suspects, The (1995)     | Crime|Mystery|Thriller             |               4.05 |\n",
      "|        24 | Powder (1995)                  | Drama|Sci-Fi                       |               3.91 |\n",
      "|        43 | Restoration (1995)             | Drama                              |               3.72 |\n",
      "\n",
      "Collaborative Filtering Recommendations for user 1:\n",
      "|   movieId | title                                             | genres                                                    |   predicted_rating |\n",
      "|----------:|:--------------------------------------------------|:----------------------------------------------------------|-------------------:|\n",
      "|      1073 | Willy Wonka & the Chocolate Factory (1971)        | Children|Comedy|Fantasy|Musical                           |               3.78 |\n",
      "|      1206 | Clockwork Orange, A (1971)                        | Crime|Drama|Sci-Fi|Thriller                               |               3.48 |\n",
      "|      1210 | Star Wars: Episode VI - Return of the Jedi (1983) | Action|Adventure|Sci-Fi                                   |               4.23 |\n",
      "|      1275 | Highlander (1986)                                 | Action|Adventure|Fantasy                                  |               4.04 |\n",
      "|      1617 | L.A. Confidential (1997)                          | Crime|Film-Noir|Mystery|Thriller                          |               3.74 |\n",
      "|      2005 | Goonies, The (1985)                               | Action|Adventure|Children|Comedy|Fantasy                  |               3.81 |\n",
      "|      2161 | NeverEnding Story, The (1984)                     | Adventure|Children|Fantasy                                |               3.97 |\n",
      "|      2987 | Who Framed Roger Rabbit? (1988)                   | Adventure|Animation|Children|Comedy|Crime|Fantasy|Mystery |               3.63 |\n",
      "|      3793 | X-Men (2000)                                      | Action|Adventure|Sci-Fi                                   |               4.09 |\n",
      "|      6874 | Kill Bill: Vol. 1 (2003)                          | Action|Crime|Thriller                                     |               3.88 |\n",
      "\n",
      "Matrix Factorization Recommendations for user 1:\n",
      "|   movieId | title                        | genres               |   predicted_rating |\n",
      "|----------:|:-----------------------------|:---------------------|-------------------:|\n",
      "|     43682 | I Could Go on Singing (1963) | Drama|Musical        |               3.56 |\n",
      "|     44189 | Ask the Dust (2006)          | Drama|Romance        |               3.27 |\n",
      "|     44892 | I Love Your Work (2003)      | Drama|Mystery        |               3.46 |\n",
      "|     45170 | Mata Hari (1931)             | Drama|Romance        |               3.8  |\n",
      "|     45259 | Man Push Cart (2005)         | Drama                |               3.67 |\n",
      "|     45508 | Wah-Wah (2005)               | Drama                |               3.58 |\n",
      "|     45527 | Standing Still (2005)        | Comedy|Drama|Romance |               3.34 |\n",
      "|     45837 | Let It Be (1970)             | Documentary          |               3.67 |\n",
      "|     46154 | War Within, The (2005)       | Drama                |               3.81 |\n",
      "|     46231 | Stoned (2005)                | Drama                |               3.13 |\n",
      "\n",
      "Hybrid Recommendations for user 1 and 'Toy Story (1995)':\n",
      "Sizes - Content: torch.Size([26569]), Collab: torch.Size([26569]), MF: torch.Size([26569])\n",
      "|   movieId | title                              | genres                                      |   scores |   content_score |   collab_score |   mf_score |   predicted_rating |\n",
      "|----------:|:-----------------------------------|:--------------------------------------------|---------:|----------------:|---------------:|-----------:|-------------------:|\n",
      "|         1 | Toy Story (1995)                   | Adventure|Animation|Children|Comedy|Fantasy |      nan |             nan |       0.467204 |   0.140364 |               4.01 |\n",
      "|         3 | Grumpier Old Men (1995)            | Comedy|Romance                              |      nan |             nan |       0.66112  |   0.876867 |               3.36 |\n",
      "|         4 | Waiting to Exhale (1995)           | Comedy|Drama|Romance                        |      nan |             nan |       0.778436 |   0.860745 |               3.29 |\n",
      "|         5 | Father of the Bride Part II (1995) | Comedy                                      |      nan |             nan |       0.863432 |   0.872507 |               3.45 |\n",
      "|         6 | Heat (1995)                        | Action|Crime|Thriller                       |      nan |             nan |       0.778524 |   0.899922 |               3.73 |\n",
      "|         7 | Sabrina (1995)                     | Comedy|Romance                              |      nan |             nan |       0.518256 |   0.835802 |               3.7  |\n",
      "|         8 | Tom and Huck (1995)                | Adventure|Children                          |      nan |             nan |       0.576203 |   0.856701 |               3.54 |\n",
      "|         9 | Sudden Death (1995)                | Action                                      |      nan |             nan |       0.584907 |   0.822912 |               3.51 |\n",
      "|        10 | GoldenEye (1995)                   | Action|Adventure|Thriller                   |      nan |             nan |       0.590847 |   0.881728 |               3.7  |\n",
      "|    131072 | Jesus liebt mich (2012)            | Comedy                                      |      nan |             nan |       0.76623  |   0.967061 |               3.49 |\n"
     ]
    }
   ],
   "source": [
    "# Set pandas display options for wide tables\n",
    "from tabulate import tabulate\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def predict_ratings(user_id, movie_ids):\n",
    "    \"\"\"Predict ratings for given user and movies using SVD model\"\"\"\n",
    "    predictions = []\n",
    "    for movie_id in movie_ids:\n",
    "        try:\n",
    "            pred = svd_model.predict(user_id, movie_id).est\n",
    "            predictions.append(round(pred, 2))\n",
    "        except:\n",
    "            predictions.append(None)\n",
    "    return predictions\n",
    "\n",
    "def display_recommendations(df, show_index=False, user_id=None):\n",
    "    \"\"\"Display recommendations with predicted ratings if user_id is provided\"\"\"\n",
    "    if user_id is not None and 'movieId' in df.columns:\n",
    "        df = df.copy()\n",
    "        df['predicted_rating'] = predict_ratings(user_id, df['movieId'])\n",
    "    print(tabulate(df, headers='keys', tablefmt='pipe', showindex=show_index))\n",
    "\n",
    "# Example usage with recommendations and predicted ratings\n",
    "user_id = 1\n",
    "movie_title = 'Toy Story (1995)'\n",
    "\n",
    "print(\"Popular Movies:\")\n",
    "display_recommendations(popularity_recommender(), user_id=user_id)\n",
    "\n",
    "print(\"\\nContent-based Recommendations for 'Toy Story (1995)':\")\n",
    "display_recommendations(content_based_recommender(movie_title), user_id=user_id)\n",
    "\n",
    "print(\"\\nCollaborative Filtering Recommendations for user 1:\")\n",
    "display_recommendations(collaborative_recommender(user_id), user_id=user_id)\n",
    "\n",
    "print(\"\\nMatrix Factorization Recommendations for user 1:\")\n",
    "display_recommendations(matrix_factorization_recommender(user_id), user_id=user_id)\n",
    "\n",
    "print(\"\\nHybrid Recommendations for user 1 and 'Toy Story (1995)':\")\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "loop = asyncio.get_event_loop()\n",
    "result = loop.run_until_complete(hybrid_recommender(user_id, movie_title))\n",
    "display_recommendations(result, user_id=user_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
